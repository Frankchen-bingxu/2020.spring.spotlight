{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight on FastText:\n",
    "\n",
    "-- Aswin Periyadan Kadinjapali\n",
    "\n",
    "FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.\n",
    "\n",
    "To install FastText use :\n",
    "\n",
    "- pip install fasttext\n",
    "\n",
    "if above method dosenot work type the code below-\n",
    "\n",
    "-git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "-cd fastText\n",
    "\n",
    "-make\n",
    "\n",
    "To check if fasttext has been install properly try importing it in your python notebook as shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is FastText different from gensim Word Vectors?\n",
    "\n",
    "FastText differs in the sense that word vectors a.k.a word2vec treats every single word as the smallest unit whose vector representation is to be found, but FastText assumes a word to be formed by a n-grams of character, for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny] etc, where n could range from 1 to the length of the word. This new representation of word by fastText provides the following benefits over word2vec or glove.\n",
    "\n",
    "1. Find the vector representation for rare words.\n",
    "\n",
    "2. It can give the vector representations for the words not present in the dictionary (OOV words) since these can also be broken down into character n-grams. word2vec and glove both fail to provide any vector representations for words not in the dictionary.\n",
    "\n",
    "3. Character n-grams embeddings tend to perform superior to word2vec and glove on smaller datasets.\n",
    "\n",
    "Fasttext can learn word representation using primarily two methods used to develop word vectors – Skipgram and CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sample text file being used from here: https://drive.google.com/file/d/0B8kCsmD4QAr7SC1wV3N2c1N3TkE/view?usp=sharing\n",
    "# you can also use any text file of your choice to train the word representation model using fasttext.\n",
    "'''\n",
    "file.txt is a training file containing utf-8 encoded text.\n",
    "The returned model object represents your learned model, and you can use it to retrieve information.\n",
    "'''\n",
    "#Skipgram\n",
    "model = fasttext.train_unsupervised('file.txt', model='skipgram')\n",
    "\n",
    "#CBOW\n",
    "# model = fasttext.train_unsupervised('file.txt', model='cbow')   // uncomment to run CBOW instead of Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a model object:\n",
    "You can save your trained model object by calling the function save_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the model\n",
    "model.save_model(\"model.bin\") #model.bin is the name of the model we want to save as.\n",
    "\n",
    "# retrieve saved model into model object.\n",
    "model = fasttext.load_model(\"model_filename.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of words in dictionary and  their vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words in Dictionary: \n",
      " ['the', '</s>', 'of', 'and', 'a', 'to', 'in', 'is', 'that', 'model', 'for', 'words', 'as', 'be', 'vector', 'The', 'word', 'can', 'with', 'word2vec', 'are', 'by', 'context', 'syntactic', 'semantic', 'number', 'on', 'Word2vec', 'quality', 'which', 'similar', 'more', 'window', 'words.', 'or', 'training', 'skip-gram', 'test', 'accuracy']\n",
      "\n",
      " Word vector of 'test' : \n",
      " [-3.08876904e-03  1.40953832e-03  5.73646976e-03 -4.56001202e-04\n",
      "  9.79778240e-04  3.26184067e-03 -5.62256295e-03 -4.54788515e-03\n",
      " -3.70595662e-04  6.51564449e-03 -2.73235748e-03  4.77137015e-04\n",
      "  7.69778999e-05  2.12496007e-03 -1.21135935e-02  4.71472507e-04\n",
      " -1.07827218e-04 -1.59196241e-03  8.50300398e-03  8.24919343e-03\n",
      "  6.37742365e-03 -3.52037488e-04 -6.14045374e-03 -5.98106380e-05\n",
      " -2.49465695e-03  3.22649628e-03 -2.52983358e-04 -3.95103637e-03\n",
      " -5.15940250e-04 -6.58672582e-03 -8.85066669e-03  2.23105447e-03\n",
      "  3.99405416e-03 -9.83050559e-04  1.19479769e-03  1.46879244e-03\n",
      "  4.82914550e-03 -1.59631064e-03  1.46226014e-03  2.61017750e-03\n",
      " -4.16054297e-03  4.43188474e-03  7.01625831e-05 -2.12810864e-03\n",
      "  5.20923780e-03  4.20651631e-03 -2.87018833e-03 -2.04015523e-05\n",
      " -4.55572596e-03  6.27745362e-03  2.92762788e-03 -1.09010015e-03\n",
      "  6.12175744e-03 -2.22915621e-03  4.77723032e-03  5.03728818e-03\n",
      "  2.41908338e-03 -2.08523986e-03  5.46777062e-03 -6.98767602e-03\n",
      "  8.20616633e-03  9.45053762e-04  1.40680850e-03  4.65481821e-03\n",
      "  5.84622263e-04  6.44829636e-03  2.46878044e-04  3.03797703e-03\n",
      " -5.27281314e-03  2.65544141e-03 -6.22374332e-03  3.55415931e-03\n",
      "  5.92915947e-03 -7.12637417e-03  5.48473652e-03  6.25388324e-03\n",
      " -1.80028635e-03 -2.77781510e-03  3.70402075e-03 -7.32768327e-03\n",
      " -1.06118871e-02  5.08821756e-03 -1.92068983e-03  1.20979687e-03\n",
      "  1.12786295e-03 -5.02839033e-03  1.04496442e-03  3.71089089e-03\n",
      "  1.98974204e-03  1.03560684e-03  5.49579831e-03 -3.60081252e-03\n",
      "  1.18175289e-02  7.76101777e-04  2.92403129e-04  4.40738600e-04\n",
      "  3.36801074e-03  9.11353249e-03  3.14999255e-03  3.17447062e-04]\n"
     ]
    }
   ],
   "source": [
    "# to get the list of words in the dictionary\n",
    "print(\"List of words in Dictionary: \\n\", model.words)\n",
    "# to get the vector of any word.\n",
    "print(\"\\n Word vector of 'test' : \\n\",model[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar words\n",
    "\n",
    "You can also find the words most similar to a given word. This functionality is provided by the nn parameter. There is no python binding for this function but you can use it from command line.Let’s see how we can find the most similar words to “happy”.\n",
    "\n",
    "./fasttext nn model.bin\n",
    "\n",
    "After typing the above command, the terminal will ask you to input a query word.\n",
    "\n",
    "--> \"happy\"\n",
    "\n",
    "by 0.183204\n",
    "\n",
    "be 0.0822266\n",
    "\n",
    "training 0.0522333\n",
    "\n",
    "the 0.0404951\n",
    "\n",
    "similar 0.036328\n",
    "\n",
    "\n",
    "The above is the result returned for the most similar words to happy. Interestingly, this feature could be used to correct spellings too. For example, when you enter a wrong spelling, it shows the correct spelling of the word if it occurred in the training file.\n",
    "\n",
    "--> \"wrd\"\n",
    "\n",
    "word 0.481091\n",
    "\n",
    "words. 0.389373\n",
    "\n",
    "words 0.370469\n",
    "\n",
    "word2vec 0.354458\n",
    "\n",
    "more 0.345805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word representation of unknown or rare words:\n",
    "As mensioned before you can also query for words that did not appear in your data!(rare words). Indeed words are represented by the sum of its substrings. As long as the unknown word is made of known substrings, there is a representation of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.2415316e-03  9.0507208e-05 -1.7831399e-03  2.7298834e-04\n",
      "  1.4404415e-03 -1.8822071e-03 -1.5137780e-03  6.4485741e-04\n",
      "  5.0046685e-04 -1.4109930e-03 -2.7662120e-04 -1.5640133e-03\n",
      " -1.5543110e-03  1.9397345e-04 -1.1217052e-03  2.5798799e-04\n",
      " -1.2204000e-03 -7.0091290e-04 -8.1962731e-04 -3.0692064e-04\n",
      " -4.5066726e-04 -4.7739866e-04 -9.3657913e-04 -7.9209841e-04\n",
      " -6.1361020e-04  1.7475449e-04 -8.8623620e-04 -2.7449889e-04\n",
      "  6.8193569e-04  1.8674624e-04  1.8099083e-03 -1.1102229e-03\n",
      "  6.2761077e-04 -1.0808907e-03 -6.9864187e-04  1.3800139e-03\n",
      " -5.0646992e-04  2.6524949e-04  1.3766529e-03  7.5645803e-04\n",
      " -2.8954464e-04 -1.9392713e-03 -3.0345080e-04 -1.5006375e-03\n",
      " -7.3841045e-04  1.9331733e-04  6.2301355e-05  7.0502405e-04\n",
      " -1.3981971e-03 -1.4167129e-04 -1.6719601e-04  2.3558081e-04\n",
      " -1.2008219e-03 -7.8679627e-04  5.4402684e-04 -3.3277075e-04\n",
      "  1.2448290e-03 -2.6569542e-04 -8.8291609e-04 -4.8254887e-04\n",
      "  7.0571143e-04  2.0886445e-03 -4.6093765e-04  2.5633629e-04\n",
      "  1.1041756e-03 -2.0357924e-04 -5.7642674e-04  6.3661701e-04\n",
      "  8.7973906e-04  2.5381081e-04 -4.2483967e-04  5.9009122e-05\n",
      "  1.5727144e-03 -7.3260476e-04 -3.5032147e-04  1.1979953e-03\n",
      "  7.6750730e-05  3.7704990e-04 -9.8495162e-04 -5.3589582e-04\n",
      " -3.9957825e-04 -1.0802284e-03  6.4284098e-04 -3.8260955e-04\n",
      " -1.8988083e-03  5.7724933e-04 -1.5096709e-03  1.0203880e-03\n",
      " -1.2818590e-03 -7.7294116e-04 -4.1936766e-04 -5.7732876e-05\n",
      " -1.3773765e-03  1.6453448e-03  4.7432864e-04 -9.9151069e-04\n",
      " -6.1700870e-05 -1.2368941e-03  3.0970044e-04  3.0396329e-04]\n"
     ]
    }
   ],
   "source": [
    "print(model[\"spotlight\"]) # \"spotlight\" is not present in the dictionary as we can see from the words list in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification:\n",
    "Text classification is tagging each document in the text with a particular class. Sentiment analysis and email classification are classic examples of text classification.\n",
    "The data for this analysis is taken from kaggle(https://www.kaggle.com/bittlingmayer/amazonreviews).\n",
    "\n",
    "#### Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Fasttext for Text Classification\n",
    "'''\n",
    "train_supervised('filepath'):\n",
    "Train a supervised model and return a model object. The input must be a filepath. \n",
    "The input text does not need to be tokenized as per the tokenize function, but it must be preprocessed and encoded\n",
    "as UTF-8.\n",
    "'''\n",
    "# In dataset we have two labels namely label1 which are negative reviews and label2 being a positive one.\n",
    "supervised_model = fasttext.train_supervised('train.ft.txt/train.ft.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_model.save_model('sup_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of words in dictionary and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words in the dictionary(first 100): \n",
      " ['the', 'and', 'I', 'to', 'a', 'of', 'is', 'it', 'this', '</s>', 'in', 'for', 'that', 'was', 'with', 'you', 'not', 'on', 'have', 'but', 'The', 'my', 'are', 'as', 'book', 'be', 'This', 'one', 'like', 'so', 'It', 'from', 'very', 'at', 'all', 'just', 'or', 'would', 'they', 'about', 'an', 'has', 'good', 'had', 'will', 'out', 'more', 'by', 'get', 'if', 'great', 'your', 'can', 'only', 'what', 'when', 'me', 'up', 'his', 'really', 'than', 'some', 'no', 'read', 'it.', 'who', 'other', 'A', 'he', 'because', 'much', 'were', 'even', '-', 'do', 'her', \"don't\", 'time', 'been', 'first', \"it's\", 'i', 'If', 'movie', 'their', 'these', 'which', 'am', 'any', 'there', 'them', 'how', 'love', 'could', 'after', 'bought', 'buy', 'we', 'into', 'use']\n",
      "\n",
      " List of labels in the model: \n",
      " ['__label__2', '__label__1']\n"
     ]
    }
   ],
   "source": [
    "print(\"List of words in the dictionary(first 100): \\n\",supervised_model.words[:100])\n",
    "\n",
    "print(\"\\n List of labels in the model: \\n\",supervised_model.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:   400000\n",
      "Precision P@1:   0.91624\n",
      "Recall R@1:   0.91624\n"
     ]
    }
   ],
   "source": [
    "# testing the supervised learning model.\n",
    "Results = supervised_model.test(\"test.ft.txt/test.ft.txt\")\n",
    "print(\"Number of examples:  \" , Results[0])\n",
    "print(\"Precision P@1:  \", Results[1])\n",
    "print(\"Recall R@1:  \", Results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label:  __label__2  with accuracy:  0.9995657801628113\n"
     ]
    }
   ],
   "source": [
    "#Predicting labels of sentenses using trained model:\n",
    "prediction = supervised_model.predict(\"This Product is Nice. I liked it a lot\")\n",
    "print(\"Predicted Label: \",prediction[0][0],\" with accuracy: \", prediction[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Sentence Vectors (Supervised):\n",
    "This model(supervised_model) can also be used for computing the sentence vectors. Let us see how we can compute the sentence vectors by using the .get_sentence_vector('/your sentense here/')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00074453,  0.00030297, -0.03772879,  0.01707233, -0.01307769,\n",
       "       -0.01404042, -0.0051821 , -0.00590544,  0.08188382,  0.07856984,\n",
       "        0.00419959,  0.04076321, -0.02195416,  0.00213791,  0.0328033 ,\n",
       "       -0.00996574,  0.00589274,  0.00586095, -0.02743017, -0.02000796,\n",
       "        0.03998065,  0.00544461, -0.01056082,  0.03092726,  0.0228732 ,\n",
       "        0.00456057,  0.06740533,  0.00912136, -0.02507113,  0.03563398,\n",
       "        0.00396628,  0.05174385, -0.03168781, -0.06196466, -0.04221461,\n",
       "       -0.00791785,  0.02250962, -0.03556334,  0.06457132,  0.0332955 ,\n",
       "       -0.0010088 ,  0.03508161,  0.0079563 ,  0.03280217, -0.01982999,\n",
       "        0.02058018, -0.02008981,  0.04603979,  0.03482691,  0.02149935,\n",
       "       -0.0143581 ,  0.04894133, -0.01198739,  0.00143604, -0.00303581,\n",
       "       -0.017317  ,  0.04596366,  0.0090891 , -0.04792043,  0.00875898,\n",
       "       -0.01712944,  0.02453457,  0.08769146,  0.00022876, -0.05284953,\n",
       "        0.04763065,  0.01721057,  0.05324742,  0.0639897 , -0.02564174,\n",
       "       -0.02723228,  0.01569838,  0.00483   ,  0.02008272,  0.00412085,\n",
       "        0.00631106,  0.0449371 ,  0.02953148,  0.01172451, -0.02662925,\n",
       "       -0.00337503,  0.02117607, -0.00850748,  0.02290564,  0.00289586,\n",
       "       -0.03104585,  0.00579535, -0.00246877,  0.01719708, -0.04775756,\n",
       "        0.00425028,  0.01022256,  0.08253148,  0.01819473, -0.00656835,\n",
       "       -0.08336295,  0.03158744,  0.0478724 ,  0.03061574,  0.02812201],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence vector representation.\n",
    "supervised_model.get_sentence_vector(\"this is a sample sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress model files:\n",
    "When you want to save a supervised model file, fastText can compress it in order to have a much smaller model file by sacrificing only a little bit performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the previously trained `model` object, call :\n",
    "supervised_model.quantize(input='train.ft.txt/train.ft.txt', retrain=True)\n",
    "supervised_model.save_model(\"model_supervised_quantized.ftz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'model_supervised_quantized.ftz' will have a much smaller size than 'sup_model.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "The library is surprisingly very fast in comparison to other methods for achieving the same accuracy and easily being able to compute sentence Vectors(supervised). FastText works better on small datasets in comparison to gensim and FastText performs superior to gensim in terms of syntactic performance and fairs equally well in case of semantic performance.\n",
    "\n",
    "Fasttext has gained popurality recently over gensim for word representation. It's ablility to return a representation for a word that is not present in dictionary comes in handy when most other representation models fail. \n",
    "\n",
    "The ability to vectorize an entire sentence for you is one powerful feature of fasttext. This helps in easy sentence classification thats helps in doccument summerization tasks and also doccument classification tasks which are generaly difficult tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
