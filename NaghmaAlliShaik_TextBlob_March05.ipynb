{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CSCE 670 Spotlight: Text Blob</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>\n",
    "<p>TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for performing natural language processing tasks such as part-of-speech (POS) tagging, noun phrase extractions, translation, text classification, sentiment analysis, etc.\n",
    "\n",
    "TextBlob is built on NLTK and Pattern. It is a simple and easy to learn library for performing small NLP tasks. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Setup</h2>\n",
    "<p> In the command prompt, enter the following two commands. One for the installation of Textblob and another is to download the corpora for performing the textblob operations.</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ pip install -U textblob\n",
    "$ python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Features</h2>\n",
    "<p> A number of features are offered by textblob. Textblob objects behave like python strings. The below sample sentence is used to analyse the NLP tasks provided by textblob.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(\"Information storage and retrieval, the systematic process of collecting and cataloging data so that they can be located and displayed on request. Computers and data processing techniques have made possible the high-speed, selective retrieval of large amounts of information for government, commercial, and academic purposes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1 Tokenization</h3>\n",
    "<p>TextBlob helps in splitting a paragraph into sentence objects and splitting sentences into word objects. Tokenization is the basis for any NLP task. Tokens are used to find patterns in a text.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sentence Tokenization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Information storage and retrieval, the systematic process of collecting and cataloging data so that they can be located and displayed on request.\"), Sentence(\"Computers and data processing techniques have made possible the high-speed, selective retrieval of large amounts of information for government, commercial, and academic purposes.\")]\n",
      "Datatype returned:  <class 'list'>\n",
      "Datatype of each sentence:  <class 'textblob.blob.Sentence'>\n"
     ]
    }
   ],
   "source": [
    "sentences=blob.sentences\n",
    "print(sentences)\n",
    "print(\"Datatype returned: \",type(sentences))\n",
    "print(\"Datatype of each sentence: \",type(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Word Tokenization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Information', 'storage', 'and', 'retrieval', 'the', 'systematic', 'process', 'of', 'collecting', 'and', 'cataloging', 'data', 'so', 'that', 'they', 'can', 'be', 'located', 'and', 'displayed', 'on', 'request', 'Computers', 'and', 'data', 'processing', 'techniques', 'have', 'made', 'possible', 'the', 'high-speed', 'selective', 'retrieval', 'of', 'large', 'amounts', 'of', 'information', 'for', 'government', 'commercial', 'and', 'academic', 'purposes']\n",
      "Datatype returned:  <class 'textblob.blob.WordList'>\n",
      "Datatype of each word:  <class 'textblob.blob.Word'>\n"
     ]
    }
   ],
   "source": [
    "words=blob.words\n",
    "print(\"Words:\",words)\n",
    "print(\"Datatype returned: \",type(words))\n",
    "print(\"Datatype of each word: \",type(words[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is also possible to get the words belonging to a particular sentence in Textblob</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Information', 'storage', 'and', 'retrieval', 'the', 'systematic', 'process', 'of', 'collecting', 'and', 'cataloging', 'data', 'so', 'that', 'they', 'can', 'be', 'located', 'and', 'displayed', 'on', 'request']\n"
     ]
    }
   ],
   "source": [
    "words_sentence1=sentences[0].words\n",
    "print(words_sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 Part of Speech Tagging </h3>\n",
    "<p>Part of Speech tagging is used in getting the part of speech of each word in the textblob object. For instance, it will tell whether a word is a noun, verb, pronoun,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Information', 'NN'), ('storage', 'NN'), ('and', 'CC'), ('retrieval', 'NN'), ('the', 'DT'), ('systematic', 'JJ'), ('process', 'NN'), ('of', 'IN'), ('collecting', 'VBG'), ('and', 'CC'), ('cataloging', 'VBG'), ('data', 'NNS'), ('so', 'RB'), ('that', 'IN'), ('they', 'PRP'), ('can', 'MD'), ('be', 'VB'), ('located', 'VBN'), ('and', 'CC'), ('displayed', 'VBN'), ('on', 'IN'), ('request', 'NN'), ('Computers', 'NNS'), ('and', 'CC'), ('data', 'NNS'), ('processing', 'VBG'), ('techniques', 'NNS'), ('have', 'VBP'), ('made', 'VBN'), ('possible', 'JJ'), ('the', 'DT'), ('high-speed', 'NN'), ('selective', 'JJ'), ('retrieval', 'NN'), ('of', 'IN'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('information', 'NN'), ('for', 'IN'), ('government', 'NN'), ('commercial', 'JJ'), ('and', 'CC'), ('academic', 'JJ'), ('purposes', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "pos_blob=blob.tags\n",
    "print(pos_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Applications</h4>\n",
    "<p>Parts of speech is very useful in word sense disambiguation. Word sense disambiguation is identifying which meaning of a word is used in a sentence, when the word has multiple meanings.It can also be used in text to speech conversion where we need to know the POS to know how to pronounce the word.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 Noun Phrases</h3>\n",
    "<p> In part of speech tagging you get the part of speech of all words in a blob. But in noun phrases, we can get the words which are just nouns. This can be used when we want to find the main subject of the text given to us.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['information', 'systematic process', 'computers', 'data processing techniques', 'selective retrieval', 'large amounts', 'academic purposes']\n"
     ]
    }
   ],
   "source": [
    "noun_blob=blob.noun_phrases\n",
    "print(noun_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Applications</h4>\n",
    "<p>Noun phrases is useful in automatic book indexing. In this, we collect the noun phrases and do inverse document frequency to find the topics in the book and to create the summary of a content. It can also be used for adjective grouping.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.4 Lemmatization and Word Inflection</h3>\n",
    "<p> Lemmatization and Word Inflection is used to prepare the text, words for further Natural Language Processing. It is used for data cleaning. Lemmatization is reducing a word to its base form. Textblob wordlist/word object also has a function called pluralize and singularize. However, the function doesnt work properly for some words.As we can see below singularize didn't work for the word process. It simply removed one 's' from the end of the sentence.Pluralize didn't work for already pluralized word 'computers'.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGULARIZE AND PLURALIZE\n",
      "information  pluralizes to  information\n",
      "information singularizes to  information\n",
      "\n",
      "systematic process  pluralizes to  systematic processes\n",
      "systematic process singularizes to  systematic proces\n",
      "\n",
      "computers  pluralizes to  computerss\n",
      "computers singularizes to  computer\n",
      "\n",
      "data processing techniques  pluralizes to  data processing techniquess\n",
      "data processing techniques singularizes to  data processing technique\n",
      "\n",
      "selective retrieval  pluralizes to  selective retrievals\n",
      "selective retrieval singularizes to  selective retrieval\n",
      "\n",
      "large amounts  pluralizes to  large amountss\n",
      "large amounts singularizes to  large amount\n",
      "\n",
      "academic purposes  pluralizes to  academic purposess\n",
      "academic purposes singularizes to  academic purpose\n",
      "\n",
      "LEMMATIZATION\n",
      "collecting : collect\n",
      "cataloging : catalog\n",
      "be : be\n",
      "located : locate\n",
      "displayed : display\n",
      "processing : process\n",
      "made : make\n"
     ]
    }
   ],
   "source": [
    "print(\"SINGULARIZE AND PLURALIZE\")\n",
    "for word in noun_blob:  #pluralizing/singularizing only nouns\n",
    "    print(word,\" pluralizes to \",word.pluralize())\n",
    "    print(word,\"singularizes to \",word.singularize())\n",
    "    print()\n",
    "print(\"LEMMATIZATION\")\n",
    "for word,pos in pos_blob:\n",
    "    if pos=='VB'or pos=='VBN' or pos=='VBG':\n",
    "        print(word,\":\",word.lemmatize(\"v\")) #v indicates verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.5 Spelling correction and Spellcheck</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>TextBlob has an inbuilt function 'correct()' to do spelling correction. Spellcheck() is a function which gives several possible forms of the incorrectly spelled word along with the confidence parameter. Based on this confidence parameter, correct() will pick the word with the highest confidence. For example, the word \"strage\" was supposed to be corrected to \"storage\", but the confidence level of the word \"strange\" is higher than that of the word \"storage\", thus the word \"Strange\" was selected which is wrong. Most of the words with spelling errors have been corrected properly, however, some of the words had an issue.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect sentence:  Informtion strage and retrieval, the systmatic process of collectinggg and cataloging data so thait they can be locatted and displaiyed on requeist.\n",
      "\n",
      "Corrected sentence:  Information strange and retrieved, the systematic process of collecting and cataloging data so that they can be located and displayed on request.\n"
     ]
    }
   ],
   "source": [
    "incorrect_blob=TextBlob(\"Informtion strage and retrieval, the systmatic process of collectinggg and cataloging data so thait they can be locatted and displaiyed on requeist.\")\n",
    "print(\"Incorrect sentence: \",incorrect_blob)\n",
    "print()\n",
    "print(\"Corrected sentence: \",incorrect_blob.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellcheck for the word strage:  [('strange', 0.6646525679758308), ('stage', 0.32628398791540786), ('storage', 0.00906344410876133)]\n",
      "\n",
      "Spellcheck for the word retrieval: [('retrieved', 0.6666666666666666), ('retrieve', 0.3333333333333333)]\n",
      "\n",
      "Spellcheck for the word reqeuist: [('request', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "storage_word=Word('strage')\n",
    "print(\"Spellcheck for the word strage: \",storage_word.spellcheck())\n",
    "print()\n",
    "retrieval_word=Word('retrieval')\n",
    "print(\"Spellcheck for the word retrieval:\",retrieval_word.spellcheck())\n",
    "print()\n",
    "request_word=Word('reqeuist')\n",
    "print(\"Spellcheck for the word reqeuist:\",request_word.spellcheck())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.6 WordNet Integration</h3>\n",
    "<p>WordNet is the dictionary for English language, specifically designed for natural language processing. WordNet is an NLTK corpus reader.\n",
    "\n",
    "Synset instances are the groupings of words that express the same concept (synonyms). Some of the words have only one Synset and some have several. A synset is identified with a 3-part name of the form: word.pos.number\n",
    "\n",
    "'word' is the word’s morphological stem.    \n",
    "'pos' is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB.   \n",
    "'number' is the sense number, counting from 0.\n",
    "\n",
    "After getting the synonyms of a word, synset also has a function to display the definitions and examples of the different synonyms. It is also possible to get the synsets based on parts of speech using the attribute pos.\n",
    "Similarity between the words can also be found using synsets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset:  Synset('act.n.01')\n",
      "Definition: a legal document codifying the result of deliberations of a committee or society or legislative body\n",
      "Example:  []\n",
      "\n",
      "Synset:  Synset('act.n.02')\n",
      "Definition: something that people do or cause to happen\n",
      "Example:  []\n",
      "\n",
      "Synset:  Synset('act.n.03')\n",
      "Definition: a subdivision of a play or opera or ballet\n",
      "Example:  []\n",
      "\n",
      "Synset:  Synset('act.n.04')\n",
      "Definition: a short theatrical performance that is part of a longer program\n",
      "Example:  ['he did his act three times every evening', 'she had a catchy little routine', 'it was one of the best numbers he ever did']\n",
      "\n",
      "Synset:  Synset('act.n.05')\n",
      "Definition: a manifestation of insincerity\n",
      "Example:  ['he put on quite an act for her benefit']\n",
      "\n",
      "Synset:  Synset('act.v.01')\n",
      "Definition: perform an action, or work out or perform (an action)\n",
      "Example:  ['think before you act', 'We must move quickly', 'The governor should act on the new energy bill', 'The nanny acted quickly by grabbing the toddler and covering him with a wet towel']\n",
      "\n",
      "Synset:  Synset('act.v.02')\n",
      "Definition: behave in a certain manner; show a certain behavior; conduct or comport oneself\n",
      "Example:  ['You should act like an adult', \"Don't behave like a fool\", 'What makes her do this way?', 'The dog acts ferocious, but he is really afraid of people']\n",
      "\n",
      "Synset:  Synset('act.v.03')\n",
      "Definition: play a role or part\n",
      "Example:  ['Gielgud played Hamlet', 'She wants to act Lady Macbeth, but she is too young for the role', \"She played the servant to her husband's master\"]\n",
      "\n",
      "Synset:  Synset('act.v.04')\n",
      "Definition: discharge one's duties\n",
      "Example:  ['She acts as the chair', 'In what capacity are you acting?']\n",
      "\n",
      "Synset:  Synset('act.v.05')\n",
      "Definition: pretend to have certain qualities or state of mind\n",
      "Example:  ['He acted the idiot', 'She plays deaf when the news are bad']\n",
      "\n",
      "Synset:  Synset('act.v.06')\n",
      "Definition: be suitable for theatrical performance\n",
      "Example:  ['This scene acts well']\n",
      "\n",
      "Synset:  Synset('work.v.03')\n",
      "Definition: have an effect or outcome; often the one desired or expected\n",
      "Example:  [\"The voting process doesn't work as well as people thought\", 'How does your idea work in practice?', \"This method doesn't work\", 'The breaks of my new car act quickly', 'The medicine works only if you take it with a lot of water']\n",
      "\n",
      "Synset:  Synset('act.v.08')\n",
      "Definition: be engaged in an activity, often for no particular purpose other than pleasure\n",
      "Example:  []\n",
      "\n",
      "Synset:  Synset('dissemble.v.03')\n",
      "Definition: behave unnaturally or affectedly\n",
      "Example:  [\"She's just acting\"]\n",
      "\n",
      "Synset:  Synset('act.v.10')\n",
      "Definition: perform on a stage or theater\n",
      "Example:  ['She acts in this play', \"He acted in `Julius Caesar'\", \"I played in `A Christmas Carol'\"]\n",
      "\n",
      "[Synset('act.v.01'), Synset('act.v.02'), Synset('act.v.03'), Synset('act.v.04'), Synset('act.v.05'), Synset('act.v.06'), Synset('work.v.03'), Synset('act.v.08'), Synset('dissemble.v.03'), Synset('act.v.10')]\n",
      "\n",
      "[Synset('act.n.01'), Synset('act.n.02'), Synset('act.n.03'), Synset('act.n.04'), Synset('act.n.05')]\n"
     ]
    }
   ],
   "source": [
    "from textblob.wordnet import VERB\n",
    "from textblob.wordnet import NOUN\n",
    "synset_word=Word('act')\n",
    "for word in synset_word.synsets:\n",
    "    print(\"Synset: \",word)\n",
    "    print(\"Definition:\",word.definition())\n",
    "    print(\"Example: \",word.examples())\n",
    "    print()\n",
    "print(synset_word.get_synsets(pos=VERB))\n",
    "print()\n",
    "print(synset_word.get_synsets(pos=NOUN))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Applications</h4>\n",
    "<p>Synsets are useful in finding similar words, antonyms, hypernyms, hyponyms, etc and also to find similarity between the words</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.7 Language Detection and Translation</h3>\n",
    "<p>TextBlob uses google translate library to provide features like detecting language and also language translation. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: Almacenamiento y recuperación de información, el proceso sistemático de recopilación y catalogación de datos para que puedan ubicarse y visualizarse a pedido. Las computadoras y las técnicas de procesamiento de datos han hecho posible la recuperación selectiva y de alta velocidad de grandes cantidades de información para fines gubernamentales, comerciales y académicos.\n",
      "\n",
      "Arabic: تخزين المعلومات واسترجاعها ، هي العملية المنهجية لجمع البيانات وفهرستها حتى يمكن تحديد موقعها وعرضها عند الطلب. مكنت أجهزة الكمبيوتر وتقنيات معالجة البيانات من الاسترجاع الانتقائي عالي السرعة لكميات كبيرة من المعلومات للأغراض الحكومية والتجارية والأكاديمية.\n",
      "\n",
      "Simplified Chinese: 信息存储和检索，数据收集和分类的系统过程，以便可以根据需要定位和显示它们。计算机和数据处理技术使得为政府，商业和学术目的高速，选择性地检索大量信息成为可能。\n",
      "\n",
      "en\n",
      "es\n",
      "ar\n",
      "zh-CN\n"
     ]
    }
   ],
   "source": [
    "#Language translation\n",
    "to_spanish=blob.translate(to='es')\n",
    "to_arabic=blob.translate(to='ar')\n",
    "to_simplified_chinese=blob.translate(to='zh-CN')\n",
    "print(\"Spanish:\",to_spanish)\n",
    "print()\n",
    "print(\"Arabic:\",to_arabic)\n",
    "print()\n",
    "print(\"Simplified Chinese:\",to_simplified_chinese)\n",
    "print()\n",
    "#Language Detection\n",
    "print(blob.detect_language())\n",
    "print(to_spanish.detect_language())\n",
    "print(to_arabic.detect_language())\n",
    "print(to_simplified_chinese.detect_language())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.8 N-grams</h3>\n",
    "<p> This inbuilt feature of textblob is used to get n consecutive words in a sentence. In information retrieval, this is a very useful feature as consecutive words are more informative like we studied in phrase queries than separate words. In text blob, we can define how many consecutive words we need.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Information', 'storage', 'and', 'retrieval', 'the', 'systematic', 'process']),\n",
       " WordList(['storage', 'and', 'retrieval', 'the', 'systematic', 'process', 'of']),\n",
       " WordList(['and', 'retrieval', 'the', 'systematic', 'process', 'of', 'collecting']),\n",
       " WordList(['retrieval', 'the', 'systematic', 'process', 'of', 'collecting', 'and']),\n",
       " WordList(['the', 'systematic', 'process', 'of', 'collecting', 'and', 'cataloging']),\n",
       " WordList(['systematic', 'process', 'of', 'collecting', 'and', 'cataloging', 'data']),\n",
       " WordList(['process', 'of', 'collecting', 'and', 'cataloging', 'data', 'so']),\n",
       " WordList(['of', 'collecting', 'and', 'cataloging', 'data', 'so', 'that']),\n",
       " WordList(['collecting', 'and', 'cataloging', 'data', 'so', 'that', 'they']),\n",
       " WordList(['and', 'cataloging', 'data', 'so', 'that', 'they', 'can']),\n",
       " WordList(['cataloging', 'data', 'so', 'that', 'they', 'can', 'be']),\n",
       " WordList(['data', 'so', 'that', 'they', 'can', 'be', 'located']),\n",
       " WordList(['so', 'that', 'they', 'can', 'be', 'located', 'and']),\n",
       " WordList(['that', 'they', 'can', 'be', 'located', 'and', 'displayed']),\n",
       " WordList(['they', 'can', 'be', 'located', 'and', 'displayed', 'on']),\n",
       " WordList(['can', 'be', 'located', 'and', 'displayed', 'on', 'request']),\n",
       " WordList(['be', 'located', 'and', 'displayed', 'on', 'request', 'Computers']),\n",
       " WordList(['located', 'and', 'displayed', 'on', 'request', 'Computers', 'and']),\n",
       " WordList(['and', 'displayed', 'on', 'request', 'Computers', 'and', 'data']),\n",
       " WordList(['displayed', 'on', 'request', 'Computers', 'and', 'data', 'processing']),\n",
       " WordList(['on', 'request', 'Computers', 'and', 'data', 'processing', 'techniques']),\n",
       " WordList(['request', 'Computers', 'and', 'data', 'processing', 'techniques', 'have']),\n",
       " WordList(['Computers', 'and', 'data', 'processing', 'techniques', 'have', 'made']),\n",
       " WordList(['and', 'data', 'processing', 'techniques', 'have', 'made', 'possible']),\n",
       " WordList(['data', 'processing', 'techniques', 'have', 'made', 'possible', 'the']),\n",
       " WordList(['processing', 'techniques', 'have', 'made', 'possible', 'the', 'high-speed']),\n",
       " WordList(['techniques', 'have', 'made', 'possible', 'the', 'high-speed', 'selective']),\n",
       " WordList(['have', 'made', 'possible', 'the', 'high-speed', 'selective', 'retrieval']),\n",
       " WordList(['made', 'possible', 'the', 'high-speed', 'selective', 'retrieval', 'of']),\n",
       " WordList(['possible', 'the', 'high-speed', 'selective', 'retrieval', 'of', 'large']),\n",
       " WordList(['the', 'high-speed', 'selective', 'retrieval', 'of', 'large', 'amounts']),\n",
       " WordList(['high-speed', 'selective', 'retrieval', 'of', 'large', 'amounts', 'of']),\n",
       " WordList(['selective', 'retrieval', 'of', 'large', 'amounts', 'of', 'information']),\n",
       " WordList(['retrieval', 'of', 'large', 'amounts', 'of', 'information', 'for']),\n",
       " WordList(['of', 'large', 'amounts', 'of', 'information', 'for', 'government']),\n",
       " WordList(['large', 'amounts', 'of', 'information', 'for', 'government', 'commercial']),\n",
       " WordList(['amounts', 'of', 'information', 'for', 'government', 'commercial', 'and']),\n",
       " WordList(['of', 'information', 'for', 'government', 'commercial', 'and', 'academic']),\n",
       " WordList(['information', 'for', 'government', 'commercial', 'and', 'academic', 'purposes'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Applications</h4>\n",
    "<p>N-grams has a wide usage in NLP. They can be used for auto completion of sentences, automatic spell check.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.9 Sentiment Analysis</h3>\n",
    "<p> This is one of the most important feature offered by Textblob. This is used is many applications like classification of positive/negative movie reviews, positive/negative comments in tweets, etc. When you use the function 'sentiment' on a sentence, it gives the polarity and subjectivity of a sentence.</p>\n",
    "    \n",
    "<h4>Polarity: </h4><p> Polarity ranges from -1 to +1. Negative polarity value signifies that the sentence is more negative. Positive polarity value signifies that the sentence in more positive</p>\n",
    "<h4>Subjectivity: </h4><p> Subjectivity value ranges from 0 to 1. If the sentence is factual, subjectivitiy value will be more towards zero. If the sentence is an opinion or more of a personal sentence, subjectivity value will be more towards 1.</p>\n",
    "\n",
    "<h4>Pattern Analyzer and Naive Bayes Analyser</h4>\n",
    "<p>Text Blob generally uses pattern analyser to calculate the sentiment. However, we can use Naive Bayes Analyser to compute the sentiment of sentences by manually changing the analyser parameter. PatternAnalyzer is based on the pattern library and NaiveBayesAnalyzer is an NLTK classifier trained on a movie reviews corpus.\n",
    "\n",
    "Naive Bayes Analyzer outputs the classification of the sentence (positive/negative) and also outputs how positive and negative is the sentence.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pattern Analyser: \n",
      "I love reading books : Sentiment(polarity=0.5, subjectivity=0.6)\n",
      "I hate reading books : Sentiment(polarity=-0.8, subjectivity=0.9)\n",
      "Sentiment analysis is the process of determining emotions of a writer : Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "Using Naive Bayes Analyser: \n",
      "I love reading books : Sentiment(classification='pos', p_pos=0.7582388390506671, p_neg=0.24176116094933342)\n",
      "I hate reading books : Sentiment(classification='pos', p_pos=0.7424393605051768, p_neg=0.25756063949482355)\n",
      "Sentiment analysis is the process of determining emotions of a writer : Sentiment(classification='pos', p_pos=0.9530619620476906, p_neg=0.04693803795230751)\n",
      "\n",
      "Sentence: Information storage and retrieval, the systematic process of collecting and cataloging data so that they can be located and displayed on request.\n",
      "Sentiment  Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "\n",
      "Sentence: Computers and data processing techniques have made possible the high-speed, selective retrieval of large amounts of information for government, commercial, and academic purposes.\n",
      "Sentiment  Sentiment(polarity=0.05357142857142857, subjectivity=0.35714285714285715)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import Sentence  \n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "positive_sentence=TextBlob(\"I love reading books\")\n",
    "negative_sentence=TextBlob(\"I hate reading books\")\n",
    "factual_sentence=TextBlob(\"Sentiment analysis is the process of determining emotions of a writer\")\n",
    "print(\"Using Pattern Analyser: \")\n",
    "print(positive_sentence,\":\",positive_sentence.sentiment)\n",
    "print(negative_sentence,\":\",negative_sentence.sentiment)\n",
    "print(factual_sentence,\":\",factual_sentence.sentiment)   #pattern analyzer\n",
    "print()\n",
    "\n",
    "print(\"Using Naive Bayes Analyser: \")\n",
    "positive_sentence=TextBlob(\"I love reading books\",analyzer=NaiveBayesAnalyzer())\n",
    "negative_sentence=TextBlob(\"I hate reading books\",analyzer=NaiveBayesAnalyzer())\n",
    "factual_sentence=TextBlob(\"Sentiment analysis is the process of determining emotions of a writer\",analyzer=NaiveBayesAnalyzer())\n",
    "print(positive_sentence,\":\",positive_sentence.sentiment)\n",
    "print(negative_sentence,\":\",negative_sentence.sentiment)\n",
    "print(factual_sentence,\":\",factual_sentence.sentiment)   #pattern analyzer\n",
    "print()\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence:\",sentence)\n",
    "    print(\"Sentiment \",sentence.sentiment)\n",
    "    print()\n",
    "#opinion = TextBlob(\"I love reading books\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Applications</h4>\n",
    "<p>TextBlob has different models for performing sentiment analysis. You can create a custom sentiment analyser according to your application. It has many applications. Some of them are\n",
    "1.Reputation management: It is used for brand monitoring using which an organization knows what they are good at and what they are bad at and improve their products. \n",
    "2. Customer support :  An organization can do social media monitoring and provide immediate responses to negative reviews. \n",
    "3. Competitor Monitoring: An organization monitors the reviews of your competitor to improve your products.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Text Classification</h2>\n",
    "<p>Creating a custom classifier is very simple with TextBlob. We can use a NaiveBayes Classifier or Decision Tree Based Classifier to perform the text classification.</p>\n",
    "<h4>Step 1 :</h4><p>Import the dataset and store it for classification purposes. \n",
    "In this example, Names dataset from NLTK corpus is used to classify male and female names. Store the male and female names in an array using the tag 'male' and 'female' respectively.\n",
    "Shuffle the data set and construct a training set and testing set.\n",
    "</p>\n",
    "<h4>Step 2:</h4><p>Creating a custom feature extractor.\n",
    "In this example, feature extractor using the last letter of the name as a feature for classification. We can add more features to improve the classification.</p>\n",
    "<h4>Step 3:</h4><p>Calling the classifier on the trainset.\n",
    "In this example, Naives Bayes Classifier is used to train the train set and the custom feature extractor is passed as a parameter</p>\n",
    "<h4>Step 4:</h4><p>Accuracy is measured on the test set.</p>\n",
    "\n",
    "<p>Naive Bayes Classifier has another function called show_informative_features(). This function displays the top features used for classification. From the example, we can see that, if the last letter is 'a', the name is mostly a female name and if the last letter is 'k', the name is mostly a male name.</p>\n",
    "<p>The classifiers also has another function called 'update()' which can be used to update the training set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\naghm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_letter_extractor(word):\n",
    "        last_letter=word[-1]\n",
    "        feats = {}\n",
    "        feats[\"last({0})\".format(last_letter)] = True\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the names corpus: ['female.txt', 'male.txt']\n",
      "Number of male names: 2943\n",
      "Number of female names: 5001\n",
      "Length of trainset: 5957\n",
      "Length of testset: 1987\n",
      "Accuracy: 0.758933064921993\n",
      "Most Informative Features\n",
      "                 last(k) = True             male : female =     60.6 : 1.0\n",
      "                 last(a) = True           female : male   =     36.4 : 1.0\n",
      "                 last(p) = True             male : female =     15.3 : 1.0\n",
      "                 last(f) = True             male : female =     15.3 : 1.0\n",
      "                 last(v) = True             male : female =     11.9 : 1.0\n",
      "None\n",
      "Sophia is a female\n",
      "Ethan is a  male\n",
      "Sophie is a female\n",
      "Peter is a  male\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from random import shuffle \n",
    "print (\"Files in the names corpus:\",names.fileids())\n",
    "#creating a list for male names with a tag\n",
    "male_names=[]\n",
    "for name in names.words('male.txt'):\n",
    "    male_names.append((name,'male'))\n",
    "len_male=len(male_names)\n",
    "print(\"Number of male names:\",len(male_names))\n",
    "\n",
    "#creating a list for female names with a tag \n",
    "female_names=[]\n",
    "for name in names.words('female.txt'):\n",
    "    female_names.append((name,'female'))\n",
    "    len_female=len(female_names)\n",
    "print(\"Number of female names:\",len(female_names))\n",
    "\n",
    "#Shuffling the data set\n",
    "shuffle(male_names)\n",
    "shuffle(female_names)\n",
    "#creating the training and testing set \n",
    "train_male=int((3/4)*len_male)\n",
    "train_female=int((3/4)*len_female)\n",
    "train_set=male_names[0:train_male]+female_names[0:train_female]\n",
    "test_set=male_names[train_male:len_male]+female_names[train_female:len_female]\n",
    "print(\"Length of trainset:\",len(train_set))\n",
    "print(\"Length of testset:\",len(test_set))\n",
    "\n",
    "#Calling the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier(train_set,feature_extractor=last_letter_extractor)\n",
    "\n",
    "#Computing the accuracy on test set\n",
    "accuracy = classifier.accuracy(test_set)\n",
    "print(\"Accuracy:\",accuracy)\n",
    "      \n",
    "#Showing the informative features\n",
    "print (classifier.show_informative_features(5))\n",
    "      \n",
    "#Testing the classifier with random names\n",
    "word1=\"Sophia\"\n",
    "word2=\"Ethan\"\n",
    "word3=\"Sophie\"\n",
    "word4=\"Peter\"\n",
    "print(word1,\"is a\",classifier.classify(word1))\n",
    "print(word2,\"is a \",classifier.classify(word2))\n",
    "print(word3,\"is a\",classifier.classify(word3))\n",
    "print(word4,\"is a \",classifier.classify(word4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Application</h4>\n",
    "<p>Text classification is one of the important task in supervised machine learning. This can be used spam filtering, email routing, sentiment analysis etc.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Conclusion</h2>\n",
    "<p>    \n",
    "TextBlob is a simple language processing tool. The main advantage of TextBlob is that it is built on top of NLTK. It is an extension of NLTK. It makes accessing many functions of NLTK in a simplified manner. It provides a simple and easy to learn intuitive interface for beginners in contrast to NLTK. TextBlob also uses function from Pattern library and it uses google translator interface for translating languages. \n",
    "    \n",
    "It is used for text mining, text processing and text analysis. It is majorly used for its sentiment analysis function. \n",
    "It does other important text processing functions like tokenizing, lemmatizing,parts of speech tagging,noun phrase extraction, spelling correction,finding similarity between words,machine translation, etc.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. References</h2>\n",
    "<p>\n",
    "1. https://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "2. https://textblob.readthedocs.io/en/dev/classifiers.html \n",
    "3. http://www.nltk.org/howto/wordnet.html</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
