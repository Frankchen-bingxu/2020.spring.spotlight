{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Polyglot\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In today's world, the data on the internet is increasing exponentially and it is impossible to analyse this data manually. This has lead to a lot of research in the field of Text Analysis and Natural Language Processing (NLP) using computers. Using computers for analysing text can provide some useful and inspirational insights. There are vatious different libraries available for this, NLTK and SpaCy are the most famous of the bunch. However, when working with multiple languages the capabilities of these existing models are severely limited. In order to combat this limitation, Rami Al-Rfou built polyglot. \n",
    "\n",
    "Polyglot is a natural language pipeline that supports massive multilingual applications. It is very similar to Textblob, so it is very easy to learn one if you know the other. The features include:\n",
    "    \n",
    "- [Language Detection](#languageDetection)\n",
    "- [Word Tokenization and Sentence Segmentation](#tokenization)\n",
    "- [Word Embeddings](#wordEmbeddings)\n",
    "- [Part of Speech (POS) Tagging](#posTagging)\n",
    "- [Named Entity Recognition (NER)](#ner)\n",
    "- [Morphological Analysis](#morphologicalAnalysis)\n",
    "- [Transliteration](#transliteration)\n",
    "- [Sentiment Analysis](#sentimentAnalysis)\n",
    "\n",
    "This notebook will explore this features with examples in multiple languages.\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "Run the following commands to install polyglot and it's dependencies.\n",
    "Refer the following link for more info: [polyglot](https://polyglot.readthedocs.io/en/latest/Installation.html)\n",
    "\n",
    "\n",
    "```python\n",
    "pip install polyglot\n",
    "pip install PyICU\n",
    "pip install pycld2\n",
    "pip install morfessor\n",
    "```\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Before we get started, there are some models that needs to be downloaded in order for polyglot to function properly. Since this models are pretty large, they are distributed using a download manager separately. \n",
    "\n",
    "There are 2 ways of downloading the models:\n",
    "- **Interactive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"embeddings2.en\")\n",
    "downloader.download(\"pos2.en\")\n",
    "downloader.download(\"ner2.en\")\n",
    "downloader.download(\"transliteration2.ar\")\n",
    "downloader.download(\"morph2.en\")\n",
    "downloader.download(\"morph2.ar\")\n",
    "downloader.download(\"sentiment2.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Command line (bash)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "polyglot download embeddings2.en pos2.en ner2.en transliteration2.gu morph2.en morph2.ar sentiment2.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are all the available and installed models and packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default data directory (/home/phoenix1712/polyglot_data)\n",
      "======================================================================\n",
      " Data server index for <http://polyglot.cs.stonybrook.edu/~polyglot/>\n",
      "======================================================================\n",
      "Collections:\n",
      "  [ ] LANG:af............. Afrikaans            packages and models\n",
      "  [ ] LANG:als............ Alemannic            packages and models\n",
      "  [ ] LANG:am............. Amharic              packages and models\n",
      "  [ ] LANG:an............. Aragonese            packages and models\n",
      "  [P] LANG:ar............. Arabic               packages and models\n",
      "  [ ] LANG:arz............ Egyptian Arabic      packages and models\n",
      "  [ ] LANG:as............. Assamese             packages and models\n",
      "  [ ] LANG:ast............ Asturian             packages and models\n",
      "  [ ] LANG:az............. Azerbaijani          packages and models\n",
      "  [ ] LANG:ba............. Bashkir              packages and models\n",
      "  [ ] LANG:bar............ Bavarian             packages and models\n",
      "  [ ] LANG:be............. Belarusian           packages and models\n",
      "  [ ] LANG:bg............. Bulgarian            packages and models\n",
      "  [ ] LANG:bn............. Bangla               packages and models\n",
      "  [ ] LANG:bo............. Tibetan              packages and models\n",
      "  [ ] LANG:bpy............ Bishnupriya          packages and models\n",
      "  [ ] LANG:br............. Breton               packages and models\n",
      "  [ ] LANG:bs............. Bosnian              packages and models\n",
      "  [ ] LANG:ca............. Catalan              packages and models\n",
      "  [ ] LANG:ce............. Chechen              packages and models\n",
      "  [ ] LANG:ceb............ Cebuano              packages and models\n",
      "  [ ] LANG:cs............. Czech                packages and models\n",
      "  [ ] LANG:cv............. Chuvash              packages and models\n",
      "  [ ] LANG:cy............. Welsh                packages and models\n",
      "  [ ] LANG:da............. Danish               packages and models\n",
      "  [ ] LANG:de............. German               packages and models\n",
      "  [ ] LANG:diq............ Zazaki               packages and models\n",
      "  [ ] LANG:dv............. Divehi               packages and models\n",
      "  [ ] LANG:el............. Greek                packages and models\n",
      "  [P] LANG:en............. English              packages and models\n",
      "  [ ] LANG:eo............. Esperanto            packages and models\n",
      "  [ ] LANG:es............. Spanish              packages and models\n",
      "  [ ] LANG:et............. Estonian             packages and models\n",
      "  [ ] LANG:eu............. Basque               packages and models\n",
      "  [ ] LANG:fa............. Persian              packages and models\n",
      "  [ ] LANG:fi............. Finnish              packages and models\n",
      "  [ ] LANG:fo............. Faroese              packages and models\n",
      "  [ ] LANG:fr............. French               packages and models\n",
      "  [ ] LANG:fy............. Western Frisian      packages and models\n",
      "  [ ] LANG:ga............. Irish                packages and models\n",
      "  [ ] LANG:gan............ Gan Chinese          packages and models\n",
      "  [ ] LANG:gd............. Scottish Gaelic      packages and models\n",
      "  [ ] LANG:gl............. Galician             packages and models\n",
      "  [P] LANG:gu............. Gujarati             packages and models\n",
      "  [ ] LANG:gv............. Manx                 packages and models\n",
      "  [ ] LANG:he............. Hebrew               packages and models\n",
      "  [ ] LANG:hi............. Hindi                packages and models\n",
      "  [ ] LANG:hif............ Fiji Hindi           packages and models\n",
      "  [ ] LANG:hr............. Croatian             packages and models\n",
      "  [ ] LANG:hsb............ Upper Sorbian        packages and models\n",
      "  [ ] LANG:ht............. Haitian Creole       packages and models\n",
      "  [ ] LANG:hu............. Hungarian            packages and models\n",
      "  [ ] LANG:hy............. Armenian             packages and models\n",
      "  [ ] LANG:ia............. Interlingua          packages and models\n",
      "  [ ] LANG:id............. Indonesian           packages and models\n",
      "  [ ] LANG:ilo............ Iloko                packages and models\n",
      "  [ ] LANG:io............. Ido                  packages and models\n",
      "  [ ] LANG:is............. Icelandic            packages and models\n",
      "  [ ] LANG:it............. Italian              packages and models\n",
      "  [ ] LANG:ja............. Japanese             packages and models\n",
      "  [ ] LANG:jv............. Javanese             packages and models\n",
      "  [ ] LANG:ka............. Georgian             packages and models\n",
      "  [ ] LANG:kk............. Kazakh               packages and models\n",
      "  [ ] LANG:km............. Khmer                packages and models\n",
      "  [ ] LANG:kn............. Kannada              packages and models\n",
      "  [ ] LANG:ko............. Korean               packages and models\n",
      "  [ ] LANG:ku............. Kurdish              packages and models\n",
      "  [ ] LANG:ky............. Kyrgyz               packages and models\n",
      "  [ ] LANG:la............. Latin                packages and models\n",
      "  [ ] LANG:lb............. Luxembourgish        packages and models\n",
      "  [ ] LANG:li............. Limburgish           packages and models\n",
      "  [ ] LANG:lmo............ Lombard              packages and models\n",
      "  [ ] LANG:lt............. Lithuanian           packages and models\n",
      "  [ ] LANG:lv............. Latvian              packages and models\n",
      "  [ ] LANG:mg............. Malagasy             packages and models\n",
      "  [ ] LANG:mk............. Macedonian           packages and models\n",
      "  [ ] LANG:ml............. Malayalam            packages and models\n",
      "  [ ] LANG:mn............. Mongolian            packages and models\n",
      "  [ ] LANG:mr............. Marathi              packages and models\n",
      "  [ ] LANG:ms............. Malay                packages and models\n",
      "  [ ] LANG:mt............. Maltese              packages and models\n",
      "  [ ] LANG:my............. Burmese              packages and models\n",
      "  [ ] LANG:ne............. Nepali               packages and models\n",
      "  [ ] LANG:nl............. Dutch                packages and models\n",
      "  [ ] LANG:nn............. Norwegian Nynorsk    packages and models\n",
      "  [ ] LANG:no............. Norwegian            packages and models\n",
      "  [ ] LANG:oc............. Occitan              packages and models\n",
      "  [ ] LANG:or............. Odia                 packages and models\n",
      "  [ ] LANG:os............. Ossetic              packages and models\n",
      "  [ ] LANG:pa............. Punjabi              packages and models\n",
      "  [ ] LANG:pam............ Pampanga             packages and models\n",
      "  [ ] LANG:pl............. Polish               packages and models\n",
      "  [ ] LANG:pms............ Piedmontese          packages and models\n",
      "  [ ] LANG:ps............. Pashto               packages and models\n",
      "  [ ] LANG:pt............. Portuguese           packages and models\n",
      "  [ ] LANG:qu............. Quechua              packages and models\n",
      "  [ ] LANG:rm............. Romansh              packages and models\n",
      "  [ ] LANG:ro............. Romanian             packages and models\n",
      "  [ ] LANG:ru............. Russian              packages and models\n",
      "  [ ] LANG:sa............. Sanskrit             packages and models\n",
      "  [ ] LANG:sah............ Sakha                packages and models\n",
      "  [ ] LANG:scn............ Sicilian             packages and models\n",
      "  [ ] LANG:sco............ Scots                packages and models\n",
      "  [ ] LANG:se............. Northern Sami        packages and models\n",
      "  [ ] LANG:sh............. Serbo-Croatian       packages and models\n",
      "  [ ] LANG:si............. Sinhala              packages and models\n",
      "  [ ] LANG:sk............. Slovak               packages and models\n",
      "  [ ] LANG:sl............. Slovenian            packages and models\n",
      "  [ ] LANG:sq............. Albanian             packages and models\n",
      "  [ ] LANG:sr............. Serbian              packages and models\n",
      "  [ ] LANG:su............. Sundanese            packages and models\n",
      "  [ ] LANG:sv............. Swedish              packages and models\n",
      "  [ ] LANG:sw............. Swahili              packages and models\n",
      "  [ ] LANG:szl............ Silesian             packages and models\n",
      "  [ ] LANG:ta............. Tamil                packages and models\n",
      "  [ ] LANG:te............. Telugu               packages and models\n",
      "  [ ] LANG:tg............. Tajik                packages and models\n",
      "  [ ] LANG:th............. Thai                 packages and models\n",
      "  [ ] LANG:tk............. Turkmen              packages and models\n",
      "  [ ] LANG:tl............. Tagalog              packages and models\n",
      "  [ ] LANG:tr............. Turkish              packages and models\n",
      "  [ ] LANG:tt............. Tatar                packages and models\n",
      "  [ ] LANG:ug............. Uyghur               packages and models\n",
      "  [ ] LANG:uk............. Ukrainian            packages and models\n",
      "  [ ] LANG:ur............. Urdu                 packages and models\n",
      "  [ ] LANG:uz............. Uzbek                packages and models\n",
      "  [ ] LANG:vec............ Venetian             packages and models\n",
      "  [ ] LANG:vi............. Vietnamese           packages and models\n",
      "  [ ] LANG:vls............ West Flemish         packages and models\n",
      "  [ ] LANG:vo............. Volapük              packages and models\n",
      "  [ ] LANG:wa............. Walloon              packages and models\n",
      "  [ ] LANG:war............ Waray                packages and models\n",
      "  [ ] LANG:yi............. Yiddish              packages and models\n",
      "  [ ] LANG:yo............. Yoruba               packages and models\n",
      "  [ ] LANG:zh............. Chinese              packages and models\n",
      "  [ ] LANG:zhc............ Chinese Character    packages and models\n",
      "  [ ] LANG:zhw............ Chinese Word         packages and models\n",
      "  [ ] TASK:counts2........ counts2\n",
      "  [P] TASK:embeddings2.... embeddings2\n",
      "  [P] TASK:morph2......... morph2\n",
      "  [P] TASK:ner2........... ner2\n",
      "  [P] TASK:pos2........... pos2\n",
      "  [P] TASK:sentiment2..... sentiment2\n",
      "  [ ] TASK:sgns2.......... sgns2\n",
      "  [P] TASK:transliteration2 transliteration2\n",
      "  [ ] TASK:tsne2.......... tsne2\n",
      "  [ ] TASK:uniemb......... uniemb\n",
      "  [ ] TASK:unipos......... unipos\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n"
     ]
    }
   ],
   "source": [
    "downloader.list(show_packages=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='languageDetection'></a>\n",
    "## Language Detection\n",
    "\n",
    "Language detection in a multi-language text input plays a crucial role in proper tokenization and helps analyse (language specific analysis) the data in an efficient way.\n",
    "\n",
    "Polyglot can be used to detect languages present in the text. It can also detect multiple languages and give a confidence score to each detection. \n",
    "\n",
    "Sometimes, there isn't enough data to detect a language, for e.g. from a single word. In such cases, this forces the detector to switch to a best effort strategy, a warning will be thrown and the attribute reliable will be set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Arabic      code: ar       confidence:  99.0 read bytes:   907\n",
      "name: French      code: fr       confidence:  94.0 read bytes:  1204\n"
     ]
    }
   ],
   "source": [
    "# Single Language\n",
    "\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "arabic_text = u\"\"\"\n",
    "أفاد مصدر امني في قيادة عمليات صلاح الدين في العراق بأن \" القوات الامنية تتوقف لليوم\n",
    "الثالث على التوالي عن التقدم الى داخل مدينة تكريت بسبب\n",
    "انتشار قناصي التنظيم الذي يطلق على نفسه اسم \"الدولة الاسلامية\" والعبوات الناسفة\n",
    "والمنازل المفخخة والانتحاريين، فضلا عن ان القوات الامنية تنتظر وصول تعزيزات اضافية \".\n",
    "\"\"\"\n",
    "detector = Detector(arabic_text)\n",
    "print(detector.language)\n",
    "\n",
    "french_text = \"Bonjour, Mesdames.\"\n",
    "detector = Detector(french_text)\n",
    "print(detector.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: English     code: en       confidence:  87.0 read bytes:  1154\n",
      "name: Chinese     code: zh_Hant  confidence:   5.0 read bytes:  1755\n",
      "name: un          code: un       confidence:   0.0 read bytes:     0\n"
     ]
    }
   ],
   "source": [
    "# Multiple Langauages\n",
    "\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "mixed_text = u\"\"\"\n",
    "China (simplified Chinese: 中国; traditional Chinese: 中國),\n",
    "officially the People's Republic of China (PRC), is a sovereign state located in East Asia.\n",
    "\"\"\"\n",
    "\n",
    "for language in Detector(mixed_text).languages:\n",
    "    print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tokenization'></a>\n",
    "## Word Tokenization and Sentence Segmentation\n",
    "\n",
    "While working with text data, it is important to recogonize boundaries between words and sentences. Aboe mentioned techniques are used to achieve this. There are 2 ways of doing this, either by finding the boundaries of the sentence first and further breaking the sentence into words, or we could identify the words first and then segment them to make a sentence.\n",
    "\n",
    "Here are some examples in English and Mandarin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "\n",
    "text = Text(\"Australia posted a World Cup record total of 417 - 6 as they beat Afghanistan by 275 runs .\"\n",
    "\"David Warner hit 178 off 133 balls , Steve Smith scored 95 while Glenn Maxwell struck 88 in 39 deliveries in the Pool A encounter in Perth .\"\n",
    "\"Afghanistan were then dismissed for 142 , with Mitchell Johnson and Mitchell Starc taking six wickets between them .\"\n",
    "\"Australia's score surpassed the 413 - 5 India made against Bermuda in 2007 .\"\n",
    "\"It continues the pattern of bat dominating ball in this tournament as the third 400 plus score achieved in the pool stages , following South Africa's 408 - 5 and 411 - 4 against West Indies and Ireland respectively .\"\n",
    "\"The winning margin beats the 257 - run amount by which India beat Bermuda in Port of Spain in 2007 , which was equalled five days ago by South Africa in their victory over West Indies in Sydney .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Australia', 'posted', 'a', 'World', 'Cup', 'record', 'total', 'of', '417', '-', '6', 'as', 'they', 'beat', 'Afghanistan', 'by', '275', 'runs', '.', 'David', 'Warner', 'hit', '178', 'off', '133', 'balls', ',', 'Steve', 'Smith', 'scored', '95', 'while', 'Glenn', 'Maxwell', 'struck', '88', 'in', '39', 'deliveries', 'in', 'the', 'Pool', 'A', 'encounter', 'in', 'Perth', '.', 'Afghanistan', 'were', 'then', 'dismissed', 'for', '142', ',', 'with', 'Mitchell', 'Johnson', 'and', 'Mitchell', 'Starc', 'taking', 'six', 'wickets', 'between', 'them', '.', \"Australia's\", 'score', 'surpassed', 'the', '413', '-', '5', 'India', 'made', 'against', 'Bermuda', 'in', '2007', '.', 'It', 'continues', 'the', 'pattern', 'of', 'bat', 'dominating', 'ball', 'in', 'this', 'tournament', 'as', 'the', 'third', '400', 'plus', 'score', 'achieved', 'in', 'the', 'pool', 'stages', ',', 'following', 'South', \"Africa's\", '408', '-', '5', 'and', '411', '-', '4', 'against', 'West', 'Indies', 'and', 'Ireland', 'respectively', '.', 'The', 'winning', 'margin', 'beats', 'the', '257', '-', 'run', 'amount', 'by', 'which', 'India', 'beat', 'Bermuda', 'in', 'Port', 'of', 'Spain', 'in', '2007', ',', 'which', 'was', 'equalled', 'five', 'days', 'ago', 'by', 'South', 'Africa', 'in', 'their', 'victory', 'over', 'West', 'Indies', 'in', 'Sydney', '.'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Australia posted a World Cup record total of 417 - 6 as they beat Afghanistan by 275 runs .\"),\n",
       " Sentence(\"David Warner hit 178 off 133 balls , Steve Smith scored 95 while Glenn Maxwell struck 88 in 39 deliveries in the Pool A encounter in Perth .\"),\n",
       " Sentence(\"Afghanistan were then dismissed for 142 , with Mitchell Johnson and Mitchell Starc taking six wickets between them .\"),\n",
       " Sentence(\"Australia's score surpassed the 413 - 5 India made against Bermuda in 2007 .\"),\n",
       " Sentence(\"It continues the pattern of bat dominating ball in this tournament as the third 400 plus score achieved in the pool stages , following South Africa's 408 - 5 and 411 - 4 against West Indies and Ireland respectively .\"),\n",
       " Sentence(\"The winning margin beats the 257 - run amount by which India beat Bermuda in Port of Spain in 2007 , which was equalled five days ago by South Africa in their victory over West Indies in Sydney .\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence segmentation\n",
    "text.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Australia', 'posted', 'a', 'World', 'Cup', 'record', 'total', 'of', '417', '-', '6', 'as', 'they', 'beat', 'Afghanistan', 'by', '275', 'runs', '.'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words present in first sentence\n",
    "text.sentences[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Chinese     code: zh       confidence:  99.0 read bytes:  1920\n"
     ]
    }
   ],
   "source": [
    "# Mandarin\n",
    "\n",
    "text = u\"\"\"\n",
    "两个月前遭受恐怖袭击的法国巴黎的犹太超市在装修之后周日重新开放，法国内政部长以及超市的管理者都表示，这显示了生命力要比野蛮行为更强大。\n",
    "该超市1月9日遭受枪手袭击，导致4人死亡，据悉这起事件与法国《查理周刊》杂志社恐怖袭击案有关。\n",
    "\"\"\"\n",
    "text = Text(text)\n",
    "print(text.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['两', '个', '月', '前', '遭受', '恐怖', '袭击', '的', '法国', '巴黎', '的', '犹太', '超市', '在', '装修', '之后', '周日', '重新', '开放', '，', '法国', '内政', '部长', '以及', '超市', '的', '管理者', '都', '表示', '，', '这', '显示', '了', '生命力', '要', '比', '野蛮', '行为', '更', '强大', '。', '该', '超市', '1', '月', '9', '日', '遭受', '枪手', '袭击', '，', '导致', '4', '人', '死亡', '，', '据悉', '这', '起', '事件', '与', '法国', '《', '查理', '周刊', '》', '杂志', '社', '恐怖', '袭击', '案', '有关', '。'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"两个月前遭受恐怖袭击的法国巴黎的犹太超市在装修之后周日重新开放，法国内政部长以及超市的管理者都表示，这显示了生命力要比野蛮行为更强大。\"),\n",
       " Sentence(\"该超市1月9日遭受枪手袭击，导致4人死亡，据悉这起事件与法国《查理周刊》杂志社恐怖袭击案有关。\")]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence segmentation\n",
    "text.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wordEmbeddings'></a>\n",
    "## Word Embeddings\n",
    "\n",
    "Logically it would make sense for words with similar meaning to have a representation which is close. But normal one hot encoding does not achieve this and moreover it is extremely sparse. To resolve this problem, the words are represented in a space where words with similar meaning have close representation, such representations are called word embeddings. \n",
    "\n",
    "The Embedding class in polyglot can read word embeddings from different sources:\n",
    "\n",
    "+  Gensim word2vec objects: (from_gensim method)\n",
    "+ Word2vec binary/text models: (from_word2vec method)\n",
    "+ GloVe models (from_glove method)\n",
    "+ polyglot pickle files: (load method)\n",
    "\n",
    "The word embeddings are not unit vectors, actually the more frequent the word is the larger the norm of its own vector. But in most of the machine learning tasks such as classification and training of RNNs, normlised weights are required. Polyglot provides an easy way to normalise the embeddings.\n",
    "\n",
    "We can use the above download embedding model for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the previously downloaded embedding mdoel\n",
    "from polyglot.mapping import Embedding\n",
    "embeddings = Embedding.load(\"/home/phoenix1712/polyglot_data/embeddings2/en/embeddings_pkl.tar.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Visualization',\n",
       " 'Coding',\n",
       " 'Retention',\n",
       " 'Manipulation',\n",
       " 'Pricing',\n",
       " 'Validation',\n",
       " 'Estimation',\n",
       " 'Projections',\n",
       " 'Forecasting',\n",
       " 'Sampling']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get neightbours of the word \"Retrieval\" in the embedding space..\n",
    "embeddings.nearest_neighbors(\"Retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization - 1.2541\n",
      "Coding   - 1.2838\n",
      "Retention - 1.3252\n",
      "Manipulation - 1.3609\n",
      "Pricing  - 1.3930\n",
      "Validation - 1.4006\n",
      "Estimation - 1.4009\n",
      "Projections - 1.4028\n",
      "Forecasting - 1.4135\n",
      "Sampling - 1.4180\n"
     ]
    }
   ],
   "source": [
    "# list the distance of each nearest neighbour\n",
    "neighbors = embeddings.nearest_neighbors(\"Retrieval\")\n",
    "for w,d in zip(neighbors, embeddings.distances(\"Retrieval\", neighbors)):\n",
    "    print(\"{:<8} - {:.4f}\".format(w,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization - 0.5469\n",
      "Coding   - 0.5580\n",
      "Retention - 0.5964\n",
      "Manipulation - 0.6050\n",
      "Estimation - 0.6137\n",
      "Validation - 0.6140\n",
      "Forecasting - 0.6177\n",
      "Enhancement - 0.6191\n",
      "Pricing  - 0.6227\n",
      "Projections - 0.6248\n"
     ]
    }
   ],
   "source": [
    "# normalise the embedding weights and list the distance of nearest neighbours\n",
    "embeddings = embeddings.normalize_words()\n",
    "neighbors = embeddings.nearest_neighbors(\"Retrieval\")\n",
    "for w,d in zip(neighbors, embeddings.distances(\"Retrieval\", neighbors)):\n",
    "    print(\"{:<8} - {:.4f}\".format(w,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Expansion\n",
    "\n",
    "The word dictionary constructed by the embedding models does not contain all words. There are some basic epansion tools available in polyglot such as:\n",
    "- **Case Expansion** :\n",
    "It basically finds the case changed version of the word in the embedding dictionary.\n",
    "- **Digit Expansion** :\n",
    "To reduce the size of the vocabulary while training the embeddings, special classes of words are grouped. One common case of such grouping is digits. Every digit in the training corpus get replaced by the symbol #. For example, a number like 123.54 becomes ###.##. Therefore, querying the embedding for a new number like 670 will result in a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"RETRIEVAL\" is not present in the embeddings\n",
    "\"RETRIEVAL\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case Expansion\n",
    "from polyglot.mapping import CaseExpander\n",
    "embeddings.apply_expansion(CaseExpander)\n",
    "\"RETRIEVAL\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['validation',\n",
       " 'utilization',\n",
       " 'calibration',\n",
       " 'synchronization',\n",
       " 'visualization',\n",
       " 'optimization',\n",
       " 'usability',\n",
       " 'stabilization',\n",
       " 'reliability',\n",
       " 'pricing']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.nearest_neighbors(\"RETRIEVAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# digit 670 is not available in the embedding dictionary because digits are changed to # in the dictionary.\n",
    "\"670\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Digit Expansion\n",
    "from polyglot.mapping import DigitExpander\n",
    "embeddings.apply_expansion(DigitExpander)\n",
    "\"670\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##', '#', '3', '#####', '#,###', '##,###', '##EN##', '####', '###EN###', 'n']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.nearest_neighbors(\"670\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='posTagging'></a>\n",
    "## Part of Speech Tagging\n",
    "\n",
    "It is very important to know what part of language syntax structure each word in a sentence belongs to. The process of assigning a syntactic identity to a word in a sentence is called Part of Speech (POS) Tagging.\n",
    "\n",
    "Polyglot recognizes 17 parts of speech, this set is called the universal part of speech tag set:\n",
    "\n",
    "- **ADJ**: adjective\n",
    "- **ADP**: adposition\n",
    "- **ADV**: adverb\n",
    "- **AUX**: auxiliary verb\n",
    "- **CONJ**: coordinating conjunction\n",
    "- **DET**: determiner\n",
    "- **INTJ**: interjection\n",
    "- **NOUN**: noun\n",
    "- **NUM**: numeral\n",
    "- **PART**: particle\n",
    "- **PRON**: pronoun\n",
    "- **PROPN**: proper noun\n",
    "- **PUNCT**: punctuation\n",
    "- **SCONJ**: subordinating conjunction\n",
    "- **SYM**: symbol\n",
    "- **VERB**: verb\n",
    "- **X**: other\n",
    "\n",
    "The models were trained on a combination of:\n",
    "\n",
    "- Original CONLL datasets after the tags were converted using the [universal POS tables](https://universaldependencies.org/docs/tagset-conversion/index.html).\n",
    "- Universal Dependencies 1.0 corpora whenever they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Italian                    2. French                     3. Spanish; Castilian       \n",
      "  4. Bulgarian                  5. Slovene                    6. Irish                    \n",
      "  7. Finnish                    8. Dutch                      9. Swedish                  \n",
      " 10. Danish                    11. Portuguese                12. English                  \n",
      " 13. German                    14. Indonesian                15. Czech                    \n",
      " 16. Hungarian                \n"
     ]
    }
   ],
   "source": [
    "# Supported languages for POS tagging\n",
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"pos2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Texas', 'PROPN'),\n",
       " ('A', 'NOUN'),\n",
       " ('&', 'CONJ'),\n",
       " ('M', 'PROPN'),\n",
       " ('ruins', 'NOUN'),\n",
       " ('Auburn’s', 'NUM'),\n",
       " ('undefeated', 'ADJ'),\n",
       " ('home', 'NOUN'),\n",
       " ('record', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('College', 'PROPN'),\n",
       " ('Basketball', 'PROPN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the tagging for a given sentence.\n",
    "from polyglot.text import Text\n",
    "blob = \"\"\"Texas A&M ruins Auburn’s undefeated home record in College Basketball.\"\"\"\n",
    "text = Text(blob)\n",
    "text.pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ner'></a>\n",
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is the task of locating and classifying the named entities in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.. It can be thought of as a slot filling problem which requires understanding of the text syntax and structure.  \n",
    "\n",
    "Polyglot recognizes 3 categories of entities:\n",
    "\n",
    "- Locations (Tag: I-LOC): cities, countries, regions, continents, neighborhoods, administrative divisions …\n",
    "- Organizations (Tag: I-ORG): sports teams, newspapers, banks, universities, schools, non-profits, companies, …\n",
    "- Persons (Tag: I-PER): politicians, scientists, artists, atheletes …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Italian                    2. Hindi                      3. French                   \n",
      "  4. Spanish; Castilian         5. Vietnamese                 6. Arabic                   \n",
      "  7. Bulgarian                  8. Norwegian                  9. Estonian                 \n",
      " 10. Japanese                  11. Greek, Modern             12. Slovene                  \n",
      " 13. Korean                    14. Serbian                   15. Finnish                  \n",
      " 16. Catalan; Valencian        17. Croatian                  18. Dutch                    \n",
      " 19. Swedish                   20. Tagalog                   21. Danish                   \n",
      " 22. Latvian                   23. Ukrainian                 24. Romanian, Moldavian, ... \n",
      " 25. Persian                   26. Slovak                    27. Portuguese               \n",
      " 28. English                   29. Malay                     30. Polish                   \n",
      " 31. German                    32. Indonesian                33. Chinese                  \n",
      " 34. Czech                     35. Hebrew (modern)           36. Lithuanian               \n",
      " 37. Turkish                   38. Hungarian                 39. Thai                     \n",
      " 40. Russian                  \n"
     ]
    }
   ],
   "source": [
    "# Languages supported by the NER in polyglot\n",
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"ner2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I-PER(['Joe', 'Biden']),\n",
       " I-ORG(['Democratic']),\n",
       " I-PER(['Bernie', 'Sanders']),\n",
       " I-PER(['Joe', 'Biden']),\n",
       " I-PER(['Jill'])]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polyglot.text import Text\n",
    "blob = \"\"\"\n",
    "Joe Biden is leading the race for the Democratic presidential nomination in one night and supplanted Bernie Sanders as the front-runner.\n",
    "Democrats should be wary of backing Joe Biden and his ‘senior moments’\n",
    "But then a protester leaped on stage and Biden’s wife, Jill, 68, instinctively stepped between him and the lunging vegan, grabbed the woman by the wrists and shoved her away.\n",
    "\"\"\"\n",
    "text = Text(blob)\n",
    "text.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->  Joe Biden is leading the race for the Democratic presidential nomination in one night and supplanted Bernie Sanders as the front-runner. \n",
      "\n",
      "I-PER ['Joe', 'Biden']\n",
      "I-ORG ['Democratic']\n",
      "I-PER ['Bernie', 'Sanders']\n",
      "\n",
      "->  Democrats should be wary of backing Joe Biden and his ‘senior moments’ \n",
      "\n",
      "I-PER ['Joe', 'Biden']\n",
      "\n",
      "->  But then a protester leaped on stage and Biden’s wife, Jill, 68, instinctively stepped between him and the lunging vegan, grabbed the woman by the wrists and shoved her away. \n",
      "\n",
      "I-PER ['Jill']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the tokens sentence-wise\n",
    "for sent in text.sentences:\n",
    "    print(\"-> \", sent, \"\\n\")\n",
    "    for entity in sent.entities:\n",
    "        print(entity.tag, entity)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Biden is leading the race for the Democratic presidential nomination in one night and supplanted Bernie Sanders as the front-runner.\n",
      "['Bernie', 'Sanders']\n",
      "Start Word Index:  16\n",
      "End Word Index:  18\n"
     ]
    }
   ],
   "source": [
    "# locate the position of the entity within the sentence.\n",
    "sent = text.sentences[0]\n",
    "print(sent)\n",
    "bernie = sent.entities[2]\n",
    "print(bernie)\n",
    "sent.words[bernie.start: bernie.end]\n",
    "print(\"Start Word Index: \", bernie.start)\n",
    "print(\"End Word Index: \", bernie.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='morphologicalAnalysis'></a>\n",
    "## Morphological Analysis\n",
    "\n",
    "Polyglot offers trained morfessor models to generate morphemes from words. The goal of the Morpho project is to develop unsupervised data-driven methods that discover the regularities behind word forming in natural languages.\n",
    "\n",
    "Morphemes are the primitive units of syntax, the smallest individually meaningful elements in the utterances of a language. They are essential for the task of language generation and detection. They can also be used in detected the word tokens in improperly tokenised text (as shown in example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Kapampangan                2. Italian                    3. Upper Sorbian            \n",
      "  4. Sakha                      5. Hindi                      6. French                   \n",
      "  7. Spanish; Castilian         8. Vietnamese                 9. Arabic                   \n",
      " 10. Macedonian                11. Pashto, Pushto            12. Bosnian-Croatian-Serbian \n",
      " 13. Egyptian Arabic           14. Norwegian Nynorsk         15. Sundanese                \n",
      " 16. Sicilian                  17. Azerbaijani               18. Bulgarian                \n",
      " 19. Yoruba                    20. Tajik                     21. Georgian                 \n",
      " 22. Tatar                     23. Galician                  24. Malagasy                 \n",
      " 25. Uighur, Uyghur            26. Amharic                   27. Venetian                 \n",
      " 28. Yiddish                   29. Norwegian                 30. Alemannic                \n",
      " 31. Estonian                  32. West Flemish              33. Divehi; Dhivehi; Mald... \n",
      " 34. Japanese                  35. Ilokano                   36. Haitian; Haitian Creole  \n",
      " 37. Belarusian                38. Greek, Modern             39. Ossetian, Ossetic        \n",
      " 40. Welsh                     41. Malayalam                 42. Albanian                 \n",
      " 43. Marathi (Marāṭhī)         44. Armenian                  45. Slovene                  \n",
      " 46. Korean                    47. Breton                    48. Irish                    \n",
      " 49. Luxembourgish, Letzeb...  50. Bengali                   51. Serbian                  \n",
      " 52. Fiji Hindi                53. Javanese                  54. Finnish                  \n",
      " 55. Gan Chinese               56. Kirghiz, Kyrgyz           57. Catalan; Valencian       \n",
      " 58. Quechua                   59. Croatian                  60. Dutch                    \n",
      " 61. Swedish                   62. Ido                       63. Tagalog                  \n",
      " 64. Sanskrit (Saṁskṛta)       65. Piedmontese language      66. Asturian                 \n",
      " 67. Danish                    68. Cebuano                   69. Western Frisian          \n",
      " 70. Kannada                   71. Scots                     72. Maltese                  \n",
      " 73. Swahili                   74. Limburgish, Limburgan...  75. Waray-Waray              \n",
      " 76. Lombard language          77. Uzbek                     78. Kurdish                  \n",
      " 79. Latvian                   80. Burmese                   81. Aragonese                \n",
      " 82. Volapük                   83. Northern Sami             84. Faroese                  \n",
      " 85. Kazakh                    86. Telugu                    87. Ukrainian                \n",
      " 88. Assamese                  89. Chuvash                   90. Silesian                 \n",
      " 91. Turkmen                   92. Romanian, Moldavian, ...  93. Persian                  \n",
      " 94. Tibetan Standard, Tib...  95. Latin                     96. Slovak                   \n",
      " 97. Sinhala, Sinhalese        98. Bavarian                  99. Icelandic                \n",
      "100. Mongolian                101. Walloon                  102. Portuguese               \n",
      "103. Urdu                     104. Gujarati                 105. Manx                     \n",
      "106. Tamil                    107. Khmer                    108. English                  \n",
      "109. Malay                    110. Chechen                  111. Bishnupriya Manipuri     \n",
      "112. Afrikaans                113. Basque                   114. Polish                   \n",
      "115. German                   116. Esperanto                117. Indonesian               \n",
      "118. Occitan                  119. Chinese                  120. Czech                    \n",
      "121. Hebrew (modern)          122. Romansh                  123. Lithuanian               \n",
      "124. Turkish                  125. Nepali                   126. Bosnian                  \n",
      "127. Interlingua              128. Zazaki                   129. Oriya                    \n",
      "130. Hungarian                131. Scottish Gaelic; Gaelic  132. Bashkir                  \n",
      "133. Thai                     134. Panjabi, Punjabi         135. Russian                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# languages supported for morphenes creation in polyglot\n",
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"morph2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information         ['In', 'form', 'ation']\n",
      "retrieval           ['retriev', 'al']\n",
      "and                 ['and']\n",
      "storage             ['stor', 'age']\n",
      "natural             ['natural']\n",
      "language            ['language']\n",
      "processing          ['process', 'ing']\n"
     ]
    }
   ],
   "source": [
    "# generating morphemes for some common words\n",
    "from polyglot.text import Text, Word\n",
    "words = [\"Information\", \"retrieval\", \"and\", \"storage\", \"natural\", \"language\", \"processing\"]\n",
    "for w in words:\n",
    "    w = Word(w, language=\"en\")\n",
    "    print(\"{:<20}{}\".format(w, w.morphemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Spring', 'break', 'is', 'here', '.'])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using morphemes to properly tokenise a string\n",
    "blob = \"Springbreakishere.\"\n",
    "text = Text(blob)\n",
    "text.language = \"en\"\n",
    "text.morphemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transliteration'></a>\n",
    "## Transliteration\n",
    "\n",
    "Transliteration is the process of transferring a word from the alphabet of one language to another. This is a helpful tool to have to get started with tool to print words in different languages while preserving the pronouncition.\n",
    "\n",
    "polyglot offers support for almost 70 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Italian                    2. Hindi                      3. French                   \n",
      "  4. Spanish; Castilian         5. Vietnamese                 6. Arabic                   \n",
      "  7. Macedonian                 8. Bosnian-Croatian-Serbian   9. Norwegian Nynorsk        \n",
      " 10. Azerbaijani               11. Bulgarian                 12. Georgian                 \n",
      " 13. Galician                  14. Amharic                   15. Yiddish                  \n",
      " 16. Norwegian                 17. Estonian                  18. Japanese                 \n",
      " 19. Haitian; Haitian Creole   20. Belarusian                21. Greek, Modern            \n",
      " 22. Welsh                     23. Albanian                  24. Marathi (Marāṭhī)        \n",
      " 25. Armenian                  26. Slovene                   27. Korean                   \n",
      " 28. Irish                     29. Bengali                   30. Serbian                  \n",
      " 31. Finnish                   32. Catalan; Valencian        33. Croatian                 \n",
      " 34. Dutch                     35. Swedish                   36. Tagalog                  \n",
      " 37. Danish                    38. Kannada                   39. Maltese                  \n",
      " 40. Swahili                   41. Latvian                   42. Telugu                   \n",
      " 43. Ukrainian                 44. Romanian, Moldavian, ...  45. Persian                  \n",
      " 46. Latin                     47. Slovak                    48. Icelandic                \n",
      " 49. Portuguese                50. Urdu                      51. Gujarati                 \n",
      " 52. Tamil                     53. Khmer                     54. Malay                    \n",
      " 55. Afrikaans                 56. Basque                    57. Polish                   \n",
      " 58. German                    59. Esperanto                 60. Indonesian               \n",
      " 61. Chinese                   62. Czech                     63. Hebrew (modern)          \n",
      " 64. Lithuanian                65. Turkish                   66. Bosnian                  \n",
      " 67. Hungarian                 68. Thai                      69. Russian                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# language support for Transliteration\n",
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"transliteration2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "થીસ\n",
      "ીસ\n",
      "ા\n",
      "કોોલ\n",
      "ફેતુરે\n",
      "ટો\n",
      "હાવે\n",
      "ત\n",
      "યોુર\n",
      "ડીસપોસાલ\n"
     ]
    }
   ],
   "source": [
    "# use polyglot to transliterate words from english to gujarati\n",
    "from polyglot.transliteration import Transliterator\n",
    "from polyglot.text import Text\n",
    "\n",
    "blob = \"\"\"This is a cool feature to have at your disposal\"\"\"\n",
    "text = Text(blob)\n",
    "for x in text.transliterate(\"gu\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentimentAnalysis'></a>\n",
    "## Sentiment Analysis\n",
    "\n",
    "Polyglot has polarity lexicons for 136 languages. The scale of the words’ polarity consisted of three degrees: +1 for positive words, and -1 for negatives words. Neutral words will have a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Kapampangan                2. Italian                    3. Upper Sorbian            \n",
      "  4. Sakha                      5. Hindi                      6. French                   \n",
      "  7. Spanish; Castilian         8. Vietnamese                 9. Arabic                   \n",
      " 10. Macedonian                11. Pashto, Pushto            12. Bosnian-Croatian-Serbian \n",
      " 13. Egyptian Arabic           14. Norwegian Nynorsk         15. Sundanese                \n",
      " 16. Sicilian                  17. Azerbaijani               18. Bulgarian                \n",
      " 19. Yoruba                    20. Tajik                     21. Georgian                 \n",
      " 22. Tatar                     23. Galician                  24. Malagasy                 \n",
      " 25. Uighur, Uyghur            26. Amharic                   27. Venetian                 \n",
      " 28. Yiddish                   29. Norwegian                 30. Alemannic                \n",
      " 31. Estonian                  32. West Flemish              33. Divehi; Dhivehi; Mald... \n",
      " 34. Japanese                  35. Ilokano                   36. Haitian; Haitian Creole  \n",
      " 37. Belarusian                38. Greek, Modern             39. Ossetian, Ossetic        \n",
      " 40. Welsh                     41. Malayalam                 42. Albanian                 \n",
      " 43. Marathi (Marāṭhī)         44. Armenian                  45. Slovene                  \n",
      " 46. Korean                    47. Breton                    48. Irish                    \n",
      " 49. Luxembourgish, Letzeb...  50. Bengali                   51. Serbian                  \n",
      " 52. Fiji Hindi                53. Javanese                  54. Finnish                  \n",
      " 55. Gan Chinese               56. Kirghiz, Kyrgyz           57. Catalan; Valencian       \n",
      " 58. Quechua                   59. Croatian                  60. Dutch                    \n",
      " 61. Swedish                   62. Ido                       63. Tagalog                  \n",
      " 64. Sanskrit (Saṁskṛta)       65. Piedmontese language      66. Asturian                 \n",
      " 67. Danish                    68. Cebuano                   69. Western Frisian          \n",
      " 70. Kannada                   71. Scots                     72. Maltese                  \n",
      " 73. Swahili                   74. Limburgish, Limburgan...  75. Waray-Waray              \n",
      " 76. Lombard language          77. Uzbek                     78. Kurdish                  \n",
      " 79. Latvian                   80. Burmese                   81. Aragonese                \n",
      " 82. Volapük                   83. Northern Sami             84. Faroese                  \n",
      " 85. Kazakh                    86. Telugu                    87. Ukrainian                \n",
      " 88. Assamese                  89. Chuvash                   90. Silesian                 \n",
      " 91. Turkmen                   92. Romanian, Moldavian, ...  93. Persian                  \n",
      " 94. Tibetan Standard, Tib...  95. Latin                     96. Slovak                   \n",
      " 97. Sinhala, Sinhalese        98. Bavarian                  99. Icelandic                \n",
      "100. Mongolian                101. Walloon                  102. Portuguese               \n",
      "103. Urdu                     104. Gujarati                 105. Manx                     \n",
      "106. Tamil                    107. Chinese Word             108. Khmer                    \n",
      "109. English                  110. Malay                    111. Chechen                  \n",
      "112. Bishnupriya Manipuri     113. Afrikaans                114. Basque                   \n",
      "115. Polish                   116. German                   117. Esperanto                \n",
      "118. Indonesian               119. Occitan                  120. Chinese                  \n",
      "121. Czech                    122. Hebrew (modern)          123. Romansh                  \n",
      "124. Lithuanian               125. Turkish                  126. Nepali                   \n",
      "127. Bosnian                  128. Interlingua              129. Zazaki                   \n",
      "130. Oriya                    131. Hungarian                132. Scottish Gaelic; Gaelic  \n",
      "133. Bashkir                  134. Thai                     135. Panjabi, Punjabi         \n",
      "136. Russian                  \n"
     ]
    }
   ],
   "source": [
    "# language coverage\n",
    "from polyglot.downloader import downloader\n",
    "print(downloader.supported_languages_table(\"sentiment2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Polarity\n",
      "------------------------------\n",
      "Language         0\n",
      "coverage         0\n",
      "for              0\n",
      "this             0\n",
      "superb           1\n",
      "feature          0\n",
      "is               0\n",
      "amazing          1\n",
      "and              0\n",
      "terribly        -1\n",
      "slow            -1\n",
      ".                0\n"
     ]
    }
   ],
   "source": [
    "# getting polarity for the words in a sentence\n",
    "text = Text(\"Language coverage for this superb feature is amazing and terribly slow.\")\n",
    "print(\"{:<16}{}\".format(\"Word\", \"Polarity\")+\"\\n\"+\"-\"*30)\n",
    "for w in text.words:\n",
    "    print(\"{:<16}{:>2}\".format(w, w.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama gave a fantastic speech last night.\n",
      "['Barack', 'Obama']\n",
      "0.9444444444444444\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# getting polarity for the entitites in a sentence\n",
    "blob = (\"Barack Obama gave a fantastic speech last night. \"\n",
    "        \"Reports indicate he will move next to New Hampshire.\")\n",
    "text = Text(blob)\n",
    "first_sentence = text.sentences[0]\n",
    "print(first_sentence)\n",
    "first_entity = first_sentence.entities[0]\n",
    "print(first_entity)\n",
    "print(first_entity.positive_sentiment)\n",
    "print(first_entity.negative_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There are a lot of libraries available out there which has a list of feature more extensive than the one offered by polyglot. But polyglot offers this features in a plethora of languages and it is very easy to configure and use, when compared to bigshot libraries like NLTK and SpaCy. Thanks to NumPy, it also works really fast. Using polyglot is similar to spaCy – it’s very efficient, straightforward, and basically an excellent choice for projects involving a language SpaCy doesn’t support. The library stands out from the crowd also because it requests the usage of a dedicated command in the command line through the pipeline mechanisms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [https://polyglot.readthedocs.io/en/latest/Sentiment.html](https://polyglot.readthedocs.io/en/latest/Sentiment.html)\n",
    "- [https://github.com/aboSamoor/polyglot](https://github.com/aboSamoor/polyglot)\n",
    "- [https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb](https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb)\n",
    "- [https://www.pythonpodcast.com/polyglot-with-rami-al-rfou-episode-190/](https://www.pythonpodcast.com/polyglot-with-rami-al-rfou-episode-190/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
