{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotlight on beautifulSoup\n",
    "\n",
    "--Ruihong Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful soup is a tool for web crawling, which will return you a well parsed document tree so that you can easily find what you want. This spotlight will tell you how to use it, most of which can be find in https://www.crummy.com/software/BeautifulSoup/bs4/doc/. Finally, I will do the PageRank on the page graph crawled by TAMU portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should fetch the source html code for the aiming url. Here, we set TAMU potal as an example. Usually, we build the connection through the requests package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.tamu.edu/'\n",
    "\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a part of the source html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
      "\t<head>\n",
      "\t    <link href=\"https://www.tamu.edu/index.html\" rel=\"canonical\"/>\n",
      "\t    <!-- Google Tag Manager -->\n",
      "<script>// <![CDATA[\n",
      "(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=\n",
      "'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "})(window,document,'script','dataLayer','GTM-53LFZF');\n",
      "// ]]></script>\n",
      "<!-- End Google Tag Manager --> \n",
      "\t\t<meta charset=\"utf-8\"/>\n",
      "\t\t<meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "\t\t<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "\t\t<title>\n",
      "            Texas A&amp;M University, College Station, TX\n",
      "        </title>\n",
      "\t\t<meta content=\"texas a&amp;m university, texas, a&amp;m, texas a&amp;m, aggies, texas aggies, university, universities, college, colleges, higher education, college station, 12th man\" name=\"keywords\"/>\n",
      "\t\t<meta content=\"The first public institution of higher education, this flagship university provides the best return-on-investment among Texas's public schools, with almost 400 degrees.\" name=\"description\"/>\n",
      "\t\t<meta content=\"texas a&amp;m university, texas, a&amp;m, texas a&amp;m, aggies, texas aggies, university, universities, college, colleges, higher education, college station, 12th man\" name=\"DC.Subject.Keyword\"/>\n",
      "        <meta content=\"The first public institution of higher education, this flagship university provides the best return-on-investment among Texas's public schools, with almost 400 degrees.\" name=\"DC.Description\"/>\n",
      "        <link href=\"https://www.tamu.edu/index.html\" rel=\"canonical\"/>\n",
      "        <meta content=\"Home - Texas&#160;A&amp;M University, College Station, TX\" name=\"twitter:title\" property=\"og:title\"/>\n",
      "        <meta content=\"The first public institution of higher education, this flagship university provides the best return-on-investment among Texas's public schools, with almost 400 degrees.\" name=\"twitter:description\" property=\"og:description\"/>\n",
      "        <meta href=\"https://www.tamu.edu/index.html\" name=\"twitter:url\" property=\"og:url\"/>\n",
      "        \n",
      "\t\t<link href=\"https://cloud.typography.com/6280314/7070352/css/fonts.css\" rel=\"stylesheet\" type=\"text/css\"/> <link href=\"https://fonts.googleapis.com/css?family=Open+Sans:400,700,400italic,700italic\" rel=\"stylesheet\" type=\"text/css\"/> <link href=\"https://i.icomoon.io/public/762a161ee5/TamuWWW/style.css\" rel=\"stylesheet\"/> <link href=\"/assets/css/app.v37.css\" media=\"all\" rel=\"stylesheet\"/> <link href=\"/assets/css/normalize.min.css\" media=\"all\" rel=\"stylesheet\"/> <!-- [if lt IE 9]>\n",
      "<link href=\"/assets/css/ie8.css\" rel=\"stylesheet\" media=\"all\"/>\n",
      "<script src=\"/assets/css/ie8-head.js\"></script>\n",
      "<![endif]-->\n",
      "<script src=\"/assets/js/vendor/modernizr.min.js\" type=\"text/javascript\"></script>\n",
      "<script src=\"/assets/js/jquery.min.js\" type=\"text/javascript\"></script>\n",
      "<link href=\"/asset\n"
     ]
    }
   ],
   "source": [
    "print(html_doc[0:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse and build the document tree for this page by our beautiful soup.From now on, soup represents our document tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'lxml' here can be replaced by \"html.parser\", \"lxml-xml\", \"html5lib', which represent python html or xml parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will show you why the soup is called a document tree and why it is useful for crawling. Generally speaking, beautiful soup have distinguidhed all the html blocks and figure out the relationship between each blocks. you can fetch on particular block according to the html structure like a tree. \n",
    "## Tag \n",
    "Tag is an important attribute for the html block. 'a' is the tage for &lt; a href=\"contact.html\" title=\"Contact\" &gt;. The beautiful soup enable us to fetch the blocks by their tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"skipnav js-skipnav sr-only\" href=\"#contentarea\">Skip Navigation</a>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>The event will feature discussions on the new space age, research demonstrations and exhibits.</p>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you have noticed that it only return the first block with a tag. To fetch all the blocks from all depth of the tree, you need do as below. By the way, the return of the command is a list. 'a' and 'p' are two important tags. 'a' contains hyperlink, and 'p' contains the paragraph to be fetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"skipnav js-skipnav sr-only\" href=\"#contentarea\">Skip Navigation</a>,\n",
       " <a href=\"/index.html\">Texas A&amp;M University</a>,\n",
       " <a aria-controls=\"search\" aria-expanded=\"false\" aria-label=\"View search form\" class=\"icon-search large-icon icon-home\" href=\"#search\" tabindex=\"0\">\n",
       " <span class=\"home-search\">\n",
       " <em>Search</em>\n",
       " </span>\n",
       " </a>,\n",
       " <a href=\"/index.html\">\n",
       " <img alt=\"Texas A&amp;M University Logo\" src=\"/assets/images/TAM-Logo-white.png\"/>\n",
       " </a>,\n",
       " <a aria-controls=\"search\" aria-expanded=\"false\" aria-label=\"View search form\" class=\"icon-search large-icon mobile-search show-for-small-only\" tabindex=\"0\">\n",
       " <strong>\n",
       " <span class=\"sr-only\">Search</span>\n",
       " </strong>\n",
       " </a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>The event will feature discussions on the new space age, research demonstrations and exhibits.</p>,\n",
       " <p>The post <a href=\"https://today.tamu.edu/2020/02/20/texas-am-will-launch-space-lab-at-next-months-sxsw-interactive-conference/\" rel=\"nofollow\">Texas A&amp;M Will Launch ‘Space Lab’ At Next Month’s SXSW Interactive Conference</a> appeared first on <a href=\"https://today.tamu.edu\" rel=\"nofollow\">Texas A&amp;M Today</a>.</p>,\n",
       " <p>The top five teams from around the world will participate in the final pitch competition at Texas A&amp;M March 31-April 2. </p>,\n",
       " <p>The post <a href=\"https://today.tamu.edu/2020/02/19/students-take-on-global-challenges-at-third-invent-for-the-planet/\" rel=\"nofollow\">Students Take On Global Challenges At Third Invent For The Planet</a> appeared first on <a href=\"https://today.tamu.edu\" rel=\"nofollow\">Texas A&amp;M Today</a>.</p>,\n",
       " <p>The capital campaign supports Bush School of Government and Public Service scholarships, programming and faculty support, plus exhibit updates and enhancements at the Bush Library.</p>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute\n",
    "I think you must notice that each block have many attributes,such as class href, title, id and content. 'href' and 'content are two important attributes for crawling. href contain the url link for the pages you need to go to in the next step. 'content' may contain the nested blocks which waiting for your mining. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['skipnav', 'js-skipnav', 'sr-only'], 'href': '#contentarea'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch specific attributes, you can use 'get'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#contentarea'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it seems the href return us some strange link! Don't worry this means the url is relative url. \n",
    "To fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tamu.edu/#contentarea\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin  \n",
    "url_togo = urljoin(url, soup.find('a').get('href'))\n",
    "print(url_togo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make the page graph by beautiful soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to build the page graph is pretty simple, you can first set some depth threshold for your crawling, iteratively go into the link on the source page (like the depth first searching). on each page, store all the hyperlink to a dictionary, whose key represent the source and the value represent the out link pages list. After removing the duplicates from every out link pages list, the dictionary can represernt the webpages graph.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the configuration for the pagerank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin   \n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.tamu.edu/' #set the start website as tamu portal\n",
    "graph = {} #the place to store the relation of those webpages\n",
    "depth = 1 #set link depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "Notice: sometimes the connection may failed due to the verification or the anti-crawling plugin. we just abort those link. Due to the large time consumption we set the depth as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crawling(url,depth,graph):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except:\n",
    "        print('An error occurred.')\n",
    "    else:\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc, 'lxml')# obtain the parsed doc object\n",
    "        if url in graph.keys():\n",
    "            for link in soup.find_all('a'):\n",
    "                if re.findall(\"^http\", str(link.get('href'))): #check whether the link is relative address or absolutely address\n",
    "                    url_togo = str(link.get('href'))\n",
    "                    \n",
    "                else:\n",
    "                    url_togo = urljoin(url, str(link.get('href')))\n",
    "                graph[url].append(url_togo)\n",
    "                if depth > 0:\n",
    "                    crawling(url_togo,depth-1,graph)\n",
    "        else:\n",
    "            for link in soup.find_all('a'):\n",
    "                if re.findall(\"^http\", str(link.get('href'))):\n",
    "                    url_togo = str(link.get('href'))\n",
    "                else:\n",
    "                    url_togo = urljoin(url, str(link.get('href')))\n",
    "                graph[url] = []\n",
    "                graph[url].append(url_togo)\n",
    "                if depth > 0:\n",
    "                    crawling(url_togo,depth-1,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred.\n"
     ]
    }
   ],
   "source": [
    "crawling(url,depth,graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph will contain all the link crawled by beautifulsoup. The key is the source, the value is the list of all the targets in that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['https://www.tamu.edu/', 'https://www.tamu.edu/#contentarea', 'https://www.tamu.edu/index.html', 'https://www.tamu.edu/#search', 'https://www.tamu.edu/None', 'https://www.tamu.edu/about/index.html', 'https://www.tamu.edu/admissions/index.html', 'https://www.tamu.edu/academics/index.html', 'https://www.tamu.edu/athletics/index.html', 'https://www.tamu.edu/research/index.html', 'https://www.tamu.edu/student-life/index.html', 'https://www.tamu.edu/future-students/index.html', 'https://www.tamu.edu/current-students/index.html', 'https://www.tamu.edu/faculty-staff/index.html', 'https://www.tamu.edu/parents/index.html', 'https://www.tamu.edu/visitors/index.html', 'https://www.tamu.edu/former-students/index.html', 'http://leadbyexample.tamu.edu/', 'http://leadbyexample.tamu.edu', 'http://majors.tamu.edu/', 'http://ogaps.tamu.edu/Prospective-Students/Programs-and-Degrees', 'http://tuition.tamu.edu/', 'https://financialaid.tamu.edu/', 'http://corps.tamu.edu/', 'https://www.tamu.edu/veterans/', 'https://www.tamu.edu/maps/index.html', 'https://howdy.tamu.edu/', 'http://www.aggiebound.com/', 'http://stepinstandup.tamu.edu/', 'http://feedproxy.google.com/~r/tamuEventHeadlines/~3/t9eLNOcUZ6o/151918', 'http://feedproxy.google.com/~r/tamuEventHeadlines/~3/0woQ3qxYATI/152039', 'http://feedproxy.google.com/~r/tamuEventHeadlines/~3/00E3DhOlvvk/150424', 'http://feedproxy.google.com/~r/tamuEventHeadlines/~3/L5v8d8zqq6A/161687', 'http://feedproxy.google.com/~r/tamuEventHeadlines/~3/MnLYTJ5CoX0/6516149', 'http://calendar.tamu.edu/', 'https://calendar.tamu.edu/tamu/view/event/event_id/123813&', 'https://www.tamu.edu/about/at-a-glance.html', 'https://www.tamu.edu/traditions/index.html', 'https://www.tamu.edu/about/departments.html', 'http://globalsupport.tamu.edu/', 'https://abroad.tamu.edu', 'http://aggiebound.com/', 'http://admissions.tamu.edu/', 'http://catalog.tamu.edu/', 'https://tuition.tamu.edu/', 'http://accountability.tamu.edu/Recognitions', 'http://library.tamu.edu/', 'http://transport.tamu.edu/', 'http://reslife.tamu.edu/', 'http://www.supportaggieland.com/', 'https://www.tamu.edu/about/employment.html', 'http://diversity.tamu.edu/', 'https://www.tamu.edu/emergency/index.html', 'https://www.tamu.edu/contact-us/index.html', 'https://www.facebook.com/tamu', 'https://twitter.com/tamu', 'https://www.linkedin.com/edu/school?id=19491', 'https://www.youtube.com/tamu', 'https://www.reddit.com/user/tamu', 'http://www.tamug.edu/', 'http://www.qatar.tamu.edu/', 'https://www.tamu.edu/statements/index.html', 'https://tellsomebody.tamu.edu/reportingform/', 'https://rules-saps.tamu.edu/PDFs/08.01.01.M1.pdf', 'https://caps.tamu.edu', 'https://urc.tamu.edu/media/642261/NoticeOfNonDiscrimination.pdf', 'http://www.thecb.state.tx.us/apps/resumes/', 'https://accountability.tamu.edu/', 'http://www.texas.gov/', 'https://www.tsl.texas.gov/trail/index.html', 'http://publishingext.dir.texas.gov/portal/internal/resources/DocumentLibrary/State%20Website%20Linking%20and%20Privacy%20Policy.pdf', 'http://openrecords.tamu.edu/', 'https://www.tamu.edu/statements/accreditation-complaints.html', 'http://veterans.portal.texas.gov/', 'https://www.tamu.edu/statements/TAMU-CREWS.pdf'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank algorithm\n",
    "This is the same algorithm to the HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "column_names = [\"Oid\", \"Did\"]\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "for key in graph.keys():\n",
    "    for des in graph[key]:\n",
    "        df = df.append({'Oid' : key , 'Did' : des} , ignore_index=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fill_the_matrix(Ori,Des,Trans,linknum,allnode):\n",
    "    Trans[allnode[allnode == Ori].index[0],allnode[allnode == Des].index[0]] = float(1/linknum.loc[Ori])\n",
    "\n",
    "def pagerank(df):\n",
    "    allnode = pd.concat([df.Oid,df.Did],axis = 0).drop_duplicates()\n",
    "    Trans = np.zeros((allnode.count(),allnode.count()))\n",
    "    nodenum = allnode.shape[0]\n",
    "    link_unique = df[['Oid','Did']].drop_duplicates()\n",
    "    linknum = link_unique.groupby('Oid',as_index = True).size().to_frame('linknum')\n",
    "    allnode.reset_index(drop=True, inplace = True)\n",
    "    link_unique.apply(lambda x: fill_the_matrix(x['Oid'],x['Did'],Trans,linknum,allnode), axis = 1)\n",
    "    check = Trans.sum(axis = 1)\n",
    "    check = np.where(check == 0)\n",
    "    for index in check:\n",
    "        Trans[index] = np.full(nodenum,float(1/nodenum))\n",
    "    teleport = np.full((allnode.count(),allnode.count()),float(1/1003))\n",
    "    new_Trans = np.add(0.9*Trans,0.1*teleport)\n",
    "    node_pro = np.full((nodenum), float(1/nodenum))\n",
    "    iterchange = np.abs(node_pro).mean()\n",
    "#     print(type(iterchange))\n",
    "    threshold = 1/((1000)*nodenum)\n",
    "#     print(type(threshold))\n",
    "#     print(type(iterchange > threshold))\n",
    "#     if iterchange > threshold:\n",
    "#         print('Nothing happened with if')\n",
    "    while(iterchange > threshold):\n",
    "        node_pro_new = np.dot(node_pro,Trans)\n",
    "        iterchange = np.abs(node_pro_new - node_pro).mean()\n",
    "        node_pro = node_pro_new\n",
    "#         print(iterchange > threshold)\n",
    "#         print(iterchange)\n",
    "    indexsorted = np.argsort(-node_pro)\n",
    "\n",
    "    for i in indexsorted[0:10]:\n",
    "        print(\"%s - %s\" %(allnode.iloc[i],node_pro[i]))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.facebook.com/tamu - 0.429296597574961\n",
      "http://www.texas.gov/ - 0.4228073017222359\n",
      "http://admissions.tamu.edu/webmaster - 0.0007172457430466118\n",
      "mailto:sbs@tamu.edu - 0.0006683872003370104\n",
      "https://www.addthis.com/tellfriend_v2.php?v=300&winname=addthis&pub=ra-57963d7cf091a805&source=ctbx-300&lng=en&s=email&wid=5ubj&url=http%3A%2F%2Ftx.ag%2Flead&title=Transforming%20lives%20%26%20charting%20impact%20through%20education%2C%20discovery%20and%20innovation.%20%23TAMUleads%20%23LeadByExample&ate=AT-ra-57963d7cf091a805/-/-/59658c12f7344574/7&uid=59658c121654a9c9&description=From%20the%20lab%20to%20the%20field.%20From%20College%20Station%20to%20developing%20countries.%20From%20me%20to%20you.%20Letâs%20stand%20together%20and%20transform%20lives.%20%23TAMUleads%20%23LeadByExample&uud=1&ct=0&ui_email_to=&ui_email_from=&ui_email_note=&tt=0&captcha_provider=recaptcha2&pro=1&ats=imp_url%3D0%26smd%3Drsi%253D%2526gen%253D0%2526rsc%253D%2526dr%253D%2526sta%253DAT-ra-57963d7cf091a805%25252F-%25252F-%25252F59658c12f7344574%25252F1%26hideEmailSharingConfirmation%3Dfalse%26service%3Demail%26media%3Dundefined%26passthrough%3Dundefined%26email_template%3Dundefined%26email_vars%3Dundefined&atc=data_track_addressbar%3Dfalse%26data_track_clickback%3Dfalse%26username%3Dra-57963d7cf091a805%26services_exclude%3D%26services_exclude_natural%3D%26services_compact%3Dfacebook%252Ctwitter%252Cprint%252Cemail%252Cpinterest_share%252Cgmail%252Cgoogle_plusone_share%252Clinkedin%252Cmailto%252Ctumblr%252Cmore%26product%3Dctbx-300%26widgetId%3D5ubj%26pubid%3Dra-57963d7cf091a805%26ui_pane%3Demail&rb=false - 0.0006297551478927815\n",
      "https://www.tamus.edu - 0.00047089432727809993\n",
      "https://www.tamu.edu/statements/index.html#contentarea - 0.00046282219887927323\n",
      "https://accounts.google.com/ServiceLogin?hl=en&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fhl%3Den%26app%3Ddesktop%26next%3D%252Ftamu%26action_handle_signin%3Dtrue%26feature%3Dplaylist&service=youtube&uilel=3&passive=true - 0.00045560213653132064\n",
      "https://linkedin.com/help/linkedin?trk=d_checkpoint_ch_captchaV2Challenge_ft_send_feedback&lang=en - 0.00045560213653132064\n",
      "http://support.twitter.com/articles/14226-how-to-find-your-twitter-short-code-or-long-code - 0.00045560213653132064\n"
     ]
    }
   ],
   "source": [
    "pagerank(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
