{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dimensionality Reduction in Python with Scikit-Learn\n",
    "\n",
    "# 1  Introduction\n",
    "Scikit-learn is a free software machine learning library for the Python programming language. It is built on NumPy, SciPy, and matplotlib. It features various classification, regression, clustering and dimentionality reduction algorithms. In this notebook, we would focus on its use in the dimensionality reduction topic.\n",
    "\n",
    "Dimensionality reduction (DR) is embedding the original high-dimensional data in a lower-dimensional space, where critical information should be preserved. The motivation to apply DR is as follows:\n",
    "- Combat computational cost\n",
    "- Avoid curse of dimensionality\n",
    "- Capture intrinsic dimensionality\n",
    "- Noise removal \n",
    "- Visualize when data (2D or 3D)\n",
    "\n",
    "### Application in Machine Learning\n",
    "In machine learning, The model performance could become better with more features of samples, but also be more likely to overfit. DR can be used to extract effective features while **controlling overfitting** because it only preserves the most important components of the feature space and drops the other components.\n",
    "\n",
    "Dimensionality reduction can be used in both supervised and unsupervised learning contexts. \n",
    "\n",
    "|learning method |main algorithm |\n",
    "|--- | ---|\n",
    "|unsupervised learning |PCA|\n",
    "|supervised learning | LDA |\n",
    "\n",
    "### Notebook Structure\n",
    "Part 2 - Part 3 shows the most common dimensionality reduction techniques PCA and LDA, which are both statistical methods. We use the same Iris dataset in their implementation examples so that we can compare the two methods. Part 4 concludes the similarity and difference between these two techniques and Part 5 shows some useful resources about Scikit-learn.\n",
    "\n",
    "There are many other dimensionality techniques including non-negative matrix factorization (NMF) and independent component analysis (ICA).  \n",
    "See here for many other techniques: https://scikit-learn.org/stable/modules/decomposition.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Principal component analysis (PCA)\n",
    "## 2.1 Principle\n",
    "Principal Component Analysis (PCA) selects the most influential characteristics of the dataset, creates principal components based on them and then reduces the dimensionality. A general rule of thumb is to take number of principal  components that contribute to significant variance and ignore those with diminishing variance returns. In unsupervised learning, PCA can be used to extract principal components as input features.\n",
    "\n",
    "Performing PCA is a five-step process:\n",
    "1. **Normalization**  \n",
    "If there are large differences between the ranges of initial variables, those variables with larger ranges (e.g. 0-100) will dominate over those with small ranges (e.g. 0-1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\n",
    "2. **Compute covariance metrix**  \n",
    "The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other. Because sometimes, variables contain redundant information so that they are highly correlated.  \n",
    "The **covariance matrix** is a p Ã— p symmetric matrix, where p is the number of data dimensions. It has entries of covariances associated with all possible pairs of the initial variables. For example, the covariance matrix for a 3-dimensional data set with 3 variables x, y, and z is shown as follows:\n",
    "![Matrix.png](https://builtin.com/sites/default/files/styles/ckeditor_optimize/public/inline-images/Principal%20Component%20Analysis%20Covariance%20Matrix.png)  \n",
    "3. **Identify principle components**  \n",
    "Principal components are constructed as linear combinations of the initial variables, which makes them less interpretable. They are are selected on the basis of variance that they cause in the output and they are uncorrelated. Most of the information within the initial variables is squeezed or compressed into the first components, then maximum remaining information in the second and so on.   \n",
    "This can be done by computing the  eigenvectors and eigenvalues of the covariance metrix. The eigenvectors and eigenvalues comes in pair and their number is equal to the number of data dimensions.  \n",
    "*The **eigenvectors** of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. The **eigenvalues** are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component.*\n",
    "4. **Decide dimension of feature vector**  \n",
    "Computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. So the aim of this step is to choose how many important components to keep in the feature vector.\n",
    "5. **Recast the data along the principal components axes**  \n",
    "the aim of this step is to use the formed feature vector to reorient the data from the original axes to the ones represented by the principal components. This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.\n",
    "![vector.png](https://builtin.com/sites/default/files/styles/ckeditor_optimize/public/inline-images/Principal%20Component%20Analysis%20feature%20vector.png)\n",
    "\n",
    "## 2.2 Implementing PCA with Scikit-Learn\n",
    "We will follow a pipeline:\n",
    "- import libraries and dataset\n",
    "- perform preprocessing\n",
    "- perform PCA to find out optimal number of features\n",
    "- train our models with different number of principle features and make predictions\n",
    "- evaluate accuracies with different number of principle features\n",
    "\n",
    "### Importing libraries and dataset\n",
    "We are going to use the famous Iris data set. The data set contains 150 instances with 4 features, which have been equally classified into 3 classes. Each class refers to a type of iris plant. \n",
    "\n",
    "See here for more information on this dataset: https://archive.ics.uci.edu/ml/datasets/iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#download the dataset using pandas\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "\n",
    "#output the first five rows of our dataset \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since PCA can only be applied to numeric data, categorical features are required to be converted into numerical features before applying PCA by **normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal-length  sepal-width  petal-length  petal-width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns] 0         Iris-setosa\n",
      "1         Iris-setosa\n",
      "2         Iris-setosa\n",
      "3         Iris-setosa\n",
      "4         Iris-setosa\n",
      "            ...      \n",
      "145    Iris-virginica\n",
      "146    Iris-virginica\n",
      "147    Iris-virginica\n",
      "148    Iris-virginica\n",
      "149    Iris-virginica\n",
      "Name: Class, Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataset into a feature set (X) and corresponding labels (y)\n",
    "X = dataset.drop('Class', 1)\n",
    "y = dataset['Class']\n",
    "print(X, y)\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalize our feature set (X)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA\n",
    "Performing PCA using Scikit-Learn is a two-step process:\n",
    "1. Initialize the PCA class by passing the number of components to train our model.  \n",
    "Note: The n_component parameter can be set as 4 at most , which is the number of original data dimension.\n",
    "2. Call the **transform** methods by passing the feature set to these methods. The transform method returns the specified number of principal components.\n",
    "Note: PCA depends only upon the feature set and not the label data. So it's often used in unsupervised learning.\n",
    "\n",
    "**fit_transform() vs transform():**  \n",
    "The fit_transform function performs the training and transforming in one step. The reason why we use fit_transform() on the train data is that we learn the parameters of scaling on the train data while we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data. This is the standart procedure to scale. You always learn your scaling parameters on the train and then use them on the test.  \n",
    "Here is an article that explanes it very well : https://sebastianraschka.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 1) \n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class explained_variance_ratio_ of PCA returns the variance caused by each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance ratio:  [0.72226528 0.23974795 0.03338117 0.0046056 ]\n"
     ]
    }
   ],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"explained variance ratio: \", explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**: It can be seen that first principal component is responsible for 72.22% variance. Similarly, the second principal component causes 23.9% variance in the dataset. Collectively we can say that 96.21% (72.22 + 23.9) percent of the classification information contained in the feature set is captured by the first two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Making Predictions\n",
    "We'll use random forest classification to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pre = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get the result with original four features by not executing the applying PCA part\n",
    "print(pre)\n",
    "print('Accuracy', str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  1  5]]\n",
      "Accuracy 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Get the result with only one features by setting n_components = 1\n",
    "print(pre)\n",
    "print('Accuracy', str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  9  4]\n",
      " [ 0  2  4]]\n",
      "Accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "# Get the result with two features by setting n_components = 2\n",
    "print(pre)\n",
    "print('Accuracy', str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  8  5]\n",
      " [ 0  1  5]]\n",
      "Accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "# Get the result with three features by setting n_components = 3\n",
    "print(pre)\n",
    "print('Accuracy', str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  1  5]]\n",
      "Accuracy 0.9\n"
     ]
    }
   ],
   "source": [
    "# Get the result with four features by setting n_components = 4\n",
    "\n",
    "print(pre)\n",
    "print('Accuracy', str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the performance over different number of features is summarized as follow.\n",
    "\n",
    "|Principe feature number | accuracy |\n",
    "| --- | --- |\n",
    "| 0 | 1.0 |\n",
    "| 1 |  0.93 |\n",
    "| 2 | 0.8 |\n",
    "| 3 | 0.8 |\n",
    "| 4 | 0.9 |\n",
    "\n",
    "## 2.3 Result Analysis\n",
    "1. When we don't use PCA, optimal level of accuracy can be achieved. It implies dimensionality reduction techniques could lead to loss of information to some extent. Even though the accuracy could be decreased, dimensionality reduction still has meaning when we want to reduce computational cost, visualize for data analysis and control overfitting when we have too many features. \n",
    "2. When PCA is implemented, We can achieve the highest accuracy while significantly reducing the number of features in the dataset. In other words, the accuracy of a classifier doesn't necessarily improve with increased number of principal components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Linear Discriminant Analysis (LDA)\n",
    "## 3.1 Introduction\n",
    "Linear Discriminant Analysis (LDA) operates by projecting data from a multidimensional graph onto a lower-dimensional graph. LDA works best when the means of the classes are far from each other. And it can be used as a classification algorithm in addition to carrying out dimensionality reduction. \n",
    "\n",
    "Note: Not to be confused with latent Dirichlet allocation. Here is an article that explanes their difference:  \n",
    "https://www.quora.com/What-is-the-difference-between-LDA-linear-analysis-discriminant-and-LDA-latent-Dirichlet-allocation\n",
    "\n",
    "Performing LDA for a binary classification problem is a three-step process:\n",
    "1. **Normalization**: Transforming the data to comparable scales is also needed for LDA.\n",
    "2. **Plot a new axis in the 2D graph**. This new axis should separate the two data points based on two primary goals: \n",
    "    - minimizing the variance within the two classes\n",
    "    - maximizing the distance between the means of the two data classes. \n",
    "3. **Move the data points in two-dimensional graph to the new axis** in 3 steps: \n",
    "   - Calculate the separability between the classes, and this is based on the distance between the class means or the between-class variance. \n",
    "   - Calculate the within class variance, which is the distance between the mean and sample for different classes. \n",
    "   - Construct the lower dimensional (1D) space, which maximizes the between class variance according to the previous calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implementing LDA with Scikit-Learn\n",
    "We will follow a pipeline:\n",
    "\n",
    "- import libraries and dataset\n",
    "- perform preprocessing\n",
    "- perform LDA with different number of linear discriminates\n",
    "- train our models with different number of linear discriminates and make predictions\n",
    "- evaluate accuracies with different number of linear discriminates\n",
    "\n",
    "All parts are exactly the same execpt for the performing LDA part so that we can compare results of LDA and PCA in part 4.\n",
    "\n",
    "### Importing libraries and dataset\n",
    "we will also apply LDA on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#download the dataset using pandas\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "dataset = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Performing feature scaling is also needed for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into a feature set (X) and corresponding labels (y)\n",
    "X = dataset.drop('Class', 1)\n",
    "y = dataset['Class']\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalize our feature set (X)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing LDA\n",
    "Performing LDA using Scikit-Learn is a two-step process:\n",
    "1. Initialize the LDA class by passing the number of linear discriminates.  \n",
    "Note: The parameter n_components can be set as K-1 at most, where K is the classification number.\n",
    "2. Call the **transform** methods by passing the feature set to these methods. The transform method returns the specified number of linear discriminates.\n",
    "Note: The transform method here takes two parameters, which are X_train and the y_train. This reflects the fact that LDA takes the output class labels into account while selecting the linear discriminants, while PCA doesn't depend upon the output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Making Predictions\n",
    "We'll also use random forest classification to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pre = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get the result with original four features by not executing the applying LDA part\n",
    "print(pre)\n",
    "print('Accuracy ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get the result with two features by setting n_components = 1\n",
    "print(pre)\n",
    "print('Accuracy ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  1  5]]\n",
      "Accuracy 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Get the result with two features by setting n_components = 2\n",
    "print(pre)\n",
    "print('Accuracy ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the performance over different number of features is summarized as follow.  \n",
    "\n",
    "|Linear discriminates number | accuracy |\n",
    "| --- | --- |\n",
    "| 0 | 1.0 |\n",
    "| 1 | 1.0 |\n",
    "| 2 | 0.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Result Analysis\n",
    "1. With one linear discriminant, the algorithm achieved an accuracy of 100%, which is greater than the accuracy achieved with one principal component in PCA, which was 93.33%.\n",
    "2. The accuracy of a classifier doesn't necessarily improve with increased number of linear discriminates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Conclusion\n",
    "#### Similarity between PCA and LDA\n",
    "- They are both linear transformation techniques.  \n",
    "- They both use the idea of matrix factorization to reduce dimensions.  \n",
    "- They both assume that the data fit a Gaussian distribution. So they are not suitable of dimensionality reduction for non-Gaussian samples.  \n",
    "\n",
    "\n",
    "#### Difference between PCA and LDA\n",
    "- PCA is an unsupervised while LDA is a supervised dimensionality reduction technique.  \n",
    "**Advantage**: It is beneficial that PCA can be applied to labeled as well as unlabeled data since it has no concern with the class labels. On the other hand, LDA requires output classes for finding linear discriminants and hence requires labeled data.  \n",
    "**Disadvantage**: PCA cannot use the prior knowledge of experience of classification.\n",
    "- LDA can project data into at most K-1 dimension subspace, where K is the classification number. While PCA doesn't have this restriction. PCA can project data into at most N dimension subspace, where N is the original data dimension.  \n",
    "\n",
    "- PCA only considers the global structure of the data, while LDA utilizes the class information (maximum separation).  \n",
    "PCA chooses the direction of the sample point projection with the largest variance;  \n",
    "LDA mainly choose the direction with the best classification performance, seeking to maximize the distance between data points between different categories after projection and minimize the distance between data points of the same category.  \n",
    "- LDA can be used as a classification algorithm in addition to carrying out dimensionality reduction.\n",
    "- LDA may overfit the data, while PCA doesn't have this problem because it's not affected by factors other than the dataset.\n",
    "\n",
    "#### what to choose for dimensionality reduction?\n",
    "First of all, It depends on the learning method. if it's unsupervised learning, then only. PCA can be used.  \n",
    "Otherwise, it depends on the dataset you have in hand. In case of uniformly distributed data, LDA almost always performs better than PCA. However if the data is highly skewed (irregularly distributed) then it is advised to use PCA since LDA can be biased towards the majority class. And LDA does not work well when sample classification information depends on variance rather than mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Resources\n",
    "If you are interested in learning more about Scikit-Learn, checkout the homepage that includes documentation and related resources: https://scikit-learn.org/stable/\n",
    "\n",
    "Here are some useful documentation links:  \n",
    "Quick Start Tutorial http://scikit-learn.org/stable/tutorial/basic/tutorial.html  \n",
    "User Guide http://scikit-learn.org/stable/user_guide.html  \n",
    "API Reference http://scikit-learn.org/stable/modules/classes.html  \n",
    "Example Gallery http://scikit-learn.org/stable/auto_examples/index.html  \n",
    "\n",
    "### Reference:  \n",
    "https://stackabuse.com/dimensionality-reduction-in-python-with-scikit-learn/  \n",
    "https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/  \n",
    "https://stackabuse.com/implementing-lda-in-python-with-scikit-learn/  \n",
    "https://builtin.com/data-science/step-step-explanation-principal-component-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
