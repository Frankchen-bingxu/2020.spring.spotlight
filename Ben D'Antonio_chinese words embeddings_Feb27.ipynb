{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Spotlight - Ben D'Antonio\n",
    "\n",
    "*Summary of this Spotlight:* This spotlight contains information on Tencent's AI Lab Embedding Corpus for Chinese Words and Phrases. Tencent is a Chinese-centered conglomerate company with many subsidiaries specializing in Internet-related technology. Tencent has generated an NLP dataset of 8 million+ Chinese words embedded as 200 feature value vectors with Machine Learning. Features include freshness, popularity, and domain-specific features in addition to traditional embeddings features. \n",
    "\n",
    "*Contents:* This spotlight will show where to obtain the dataset and how to get started with it, demonstrate a simple model to show the dataset's effectiveness, and provide information, resources, and discussion about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Data Source URL**:\n",
    "* https://ai.tencent.com/ailab/nlp/embedding.html\n",
    "\n",
    "**Brief article discussing the dataset**:\n",
    "* https://medium.com/syncedreview/tencent-ai-lab-open-sources-8m-word-chinese-nlp-vector-dataset-564764b1abc8\n",
    "\n",
    "**A previous implementation of the dataset**:\n",
    "* https://github.com/BridgeMia/Tencent-Word2Vec-Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Tencent's Chinese Word Corpus\n",
    "\n",
    "Let's start by grabbing the file! Navigate to https://ai.tencent.com/ailab/nlp/embedding.html if you haven't yet and download the dataset. After it is completely unzipped, it should be about 13GB. Place it in the same directory as this notebook. By default, its file name should match what's written in the code below. Now you're ready!\n",
    "\n",
    "The dataset format is as follows:\n",
    "* [\\# of terms] [\\# of features]\n",
    "* [first term] [first feature] [second feature] ...\n",
    "* [second term] [first feature] [second feature] ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty simple format!\n",
    "\n",
    "The file contains 8,824,330 terms, where each term has exactly 200 feature values that line up across all terms, with each value being in the range `(-1, 1)`.\n",
    "\n",
    "We can parse it as follows into a dictionary! The database is far too large to load in its entirely here. We will use a small subset (the start) of the data. Here, we choose 10,000 for the number of entries. Despite this small size, we will see that we can still generate compelling results. After running the below code blocks, we will be able to see the indexed terms and each term's first feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numFeats = 200\n",
    "numEntries = 10000\n",
    "numFormatting = 1\n",
    "\n",
    "database = dict()\n",
    "\n",
    "def loadDB():\n",
    "    dataFile = open('Tencent_AILab_ChineseEmbedding.txt', 'r', encoding=\"utf8\")\n",
    "    cnt = 0\n",
    "    for line in dataFile:\n",
    "        if cnt > numEntries:\n",
    "            break\n",
    "        cnt += 1\n",
    "        if cnt <= numFormatting:\n",
    "            continue\n",
    "        itemList = line.split()\n",
    "        term = itemList[0]\n",
    "        itemList.pop(0)\n",
    "        database[term] = np.asarray(itemList, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadDB()\n",
    "\n",
    "for t, f in database.items():\n",
    "    print(t, f[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: If you can not see some or all Chinese characters/hanzi being output, you may have an encoding issue in your local environment. This notebook was developed on Windows. \n",
    "\n",
    "As we can see scrolling down, the dataset has catalogued just about everything, from individual characters and punctuation, to complex terms, emoticons, and even slang!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a simple model to demonstrate the dataset's embeddings. We will use a cosine similarity system with a few example words. In cosine similarity, a score of 0 means the embeddings are identical. Thus, our top results (and the words that are most similar to our chosen word) will be the embeddings with the lowest cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial as sSpatial\n",
    "\n",
    "def cosineAccessDatabase(term, numResults):\n",
    "    results = set()\n",
    "    scores = dict()\n",
    "    \n",
    "    qFeats = database[term]\n",
    "    for dTerm, dFeats in database.items():\n",
    "        if len(dFeats) != numFeats:\n",
    "            continue\n",
    "        scores[dTerm] = sSpatial.distance.cosine(qFeats, dFeats)\n",
    "        \n",
    "    topResults = []\n",
    "    i = 0\n",
    "    for t, s in sorted(scores.items(), key=lambda item: item[1]):\n",
    "        if i >= numResults:\n",
    "            break\n",
    "        if t == term:\n",
    "            continue\n",
    "        topResults.append((t, s))\n",
    "        i += 1\n",
    "        if i >= numResults:\n",
    "            break\n",
    "        \n",
    "    return topResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's look at some examples! As I don't speak Chinese, I opted to use Google translate to see what the words roughly translated to. However, the results generally demonstrate obvious connections. Examples shown were not cherry-picked for returning strong results. I just chose a few words of varying length from different parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. '电话' : \"Phone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('电话', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "* '打电话': \"Call\",\n",
    "* '短信': \"SMS\",\n",
    "* '拨打': \"Dial\",\n",
    "* '联系方式': \"contact details\",\n",
    "* '关机': \"Shutdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. '。' : \".\" (Chinese period punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('。', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "* '的': \"Of\",\n",
    "* '也': \"and also\",\n",
    "* '和': \"with\",\n",
    "* '而': \"and\",\n",
    "* ',': \",\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. '轻轻的' : \"Gently\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('轻轻的', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "* '轻轻地': \"Gently\",\n",
    "* '轻轻': \"lightly\",\n",
    "* '轻声': \"Softly\",\n",
    "* '伸手': \"Reach out\",\n",
    "* '微微': \"pico-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. '情人节' : \"Valentine's Day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('情人节', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "* '圣诞节': \"Christmas\",\n",
    "* '节日': \"festival\",\n",
    "* '生日': \"birthday\",\n",
    "* '元旦': \"New year's day\",\n",
    "* '春节': \"Chinese New Year\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. '希望能够' : \"Hope to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('希望能够', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "* '希望能': \"hope to\",\n",
    "* '希望': \"hope\",\n",
    "* '能够': \"were able\",\n",
    "* '如果能': \"If possible\",\n",
    "* '争取': \"Fight for\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "It seems that even with a simple cosine similarity model, we can generate complelling results with this dataset. The embeddings seem to clearly capture not only the notion of words that belong to similar activities but also capture more abstract semantics.\n",
    "\n",
    "* The first example shows that handling common nouns such as phones is a fairly easy task.\n",
    "* The punctuation mark results were interesting because the embeddings seem to believe punctuation and conjuctions to be extremely similar. In many sentences, a period can be substituted for a conjuction, so this is an interesting connection.\n",
    "* Clearly the system can also handle adverbs. The first three results started with the same character, 轻, which I found interesting. I'm not quite sure what to make of this due to my lack of Chinese speaking. Grabbing a prefix \"pico-\" is also notable.\n",
    "* Valentine's Day\" returned similar holidays and festivities as opposed to something like \"hearts\" or \"love.\" Perhaps none of those terms appeared in the first 10,000 entries.\n",
    "* The semantics of \"Hope to\" seem aptly captured, returning not only similar words but also the beginning of a subjective clause expressing hope (\"If possible\"). \n",
    "\n",
    "I also tested the English word \"on\" and got back other English prepositions such as \"with\" and \"and\". Lastly, I tested a Chinese emoticon ('xx'). The emoticon is generally used to express unsatisfaction with a situation. The results included '某某' (\"So and so\") and '……', suggesting that the embeddings can capture the semantics of emoticons.\n",
    "\n",
    "I regret that I could not intentionally grab slang terms to evaluate, but maybe someone who speaks Chinese well can evaluate that on their own.\n",
    "\n",
    "**And that's it!** Even with an extremely small subset of the dataset, a simple cosine model can generate compelling results. I hope this spotlight has given a thorough overview of Tencent's dataset and provided some motivation for further exploring it. \n",
    "\n",
    "Feel free to try your own or change the database size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosineAccessDatabase('[your term]', 5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
